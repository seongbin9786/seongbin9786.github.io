{"pages":[],"posts":[{"title":"Atomic Design - 1 Intro","text":"이 글은 Atomic Design의 목적과 활용에 대해 간단하게 다룬다. 이후 시리즈에서는 실제 적용 예시를 다루고자 한다. 목적현대 프론트엔드는 두 가지 복잡성을 맞이했는데, Atomic Design은 이 문제를 해결하는데 효과적이다. 이전에 비해 다양한 해상도를 지원해야 함. UI 컴포넌트의 종류가 다양해지고 복잡해짐. 많은 상태를 갖게 됨. 즉 유지보수 비용이 크게 늘어난 상태이다. 소프트웨어 개발에서 복잡성을 낮추는 방식은 분할 정복과 의존성 격리를 통한 SRP 만족인데, Atomic Design은 프론트엔드의 구성 요소를 디자인 수준에서 분할하므로, 개발자는 이를 재사용하여 총 개발 비용을 낮출 수 있다. 기존에도 HTML, CSS 수준에서 컴포넌트 개발이 어느 정도 가능했으나 React.js의 등장 후 CSS-in-JS 라는 생태계가 조성된 덕분에 컴포넌트 개념의 완전한 구현이 가능해졌기 때문에 Atomic Design이 더 많이 언급되고 활용되는 듯하다. 현대에는 디자인 시스템 (EN)을 사용하는 것이 일반적인데, 디자인 시스템이란 브랜딩 디자인, 컴포넌트를 위한 스타일 가이드, 컴포넌트 등으로 구성된 디자인 &amp; 추상적인 개념의 집합이다. 이런 디자인 시스템의 Typo, Spacing 등을 포함해 컴포넌트를 라이브러리로 개발해놓으면, 전사적으로 디자인이 통일성을 갖게 되기도 하고 개발 측면에선 이미 개발된 디자인 요소를 빠르게 선언적으로 활용할 수 있기 때문에 생산성에 많이 유리하다. 이미 널리 알려진 디자인 시스템의 라이브러리 또한 많다. 아무래도 Material Design의 구현체인 Material-UI가 가장 유명할 것이다. Atomic Design의 접근 방식은 디자인 시스템을 구축하는 데 효과적이므로 많이 사용된다. 이후에 Atomic Design이 어떻게 개발 과정에서 유용하게 사용되는지 설명한다. 이론가장 큰 개념으로 페이지, 컴포넌트가 있다. 컴포넌트가 조합돼 페이지를 구성하는 형태이다. 컴포넌트는 조합되므로 재사용된다. 즉 효율적이며 페이지 간 일관성 또한 보장된다. 컴포넌트는 아래의 4개의 하위 요소로 나뉜다. (작은 순으로 정렬) 원자 (Atom) 분자 (Molecule) 유기체 (Organism) 템플릿 (Template) 원자 (Atom)더 분해할 수 없는 가장 작은 요소이다. 아주 작은 기능도 Atom의 조합으로 만들어지는, 콘텐츠로서의 구체성은 없다. (ex) 검색 폼: { 검색 Text, 검색 Input 바, 전송 버튼 } 이라는 Atom의 조합으로 생성 (여기서 어떻게 조합하는지는 또 별개의 정보.) 종류: 버튼 텍스트 인풋, 색 팔레트, … 타이포그래피, 제목 (h1, h2, h3, …) 아이콘 체크박스, 라디오 버튼 분자 (Molecule)분자는 원자들의 조합을 그 단위로 한다. 분자부터 콘텐츠로서의 구체성이 있다. (ex) 검색 폼: { 검색 Text, 검색 Input 바, 전송 버튼 } 이라는 구체적인 목적이 있는 요소. 분자는 원자들의 조합이지만 계속해서 재사용성이 유지해야 한다. (ex) 검색 폼은 여러 페이지에서 사용됨. 아무래도 재사용성 유지를 위해 적은 수의 Atom을 조합하여 그 범용성을 높인다. 유기체 (Organism)유기체는 분자 뿐만 아니라 원자까지 조합해 만들어진다. 유기체부터는 재사용이 강제되지 않는다. 즉 분자와 유기체의 차이는 재사용성이라고 생각할 수 있다. (ex) Header, Footer 컴포넌트 유기체는 Container의 형태를 띄기도 한다. (ex) ProductList (ProductListItem의 배치 역할) 아무래도 이런 List 컴포넌트는 재사용하기 어렵기 때문에 유기체로 분류되는 것이 일반적이다. 몇 개의 Organism을 배치하면 하나의 페이지가 완성된다고 하는데 잘 모르겠다. 템플릿 (Template)하위 컴포넌트들의 배치가 목적인 컴포넌트로, 페이지 구조나 레이아웃 구성을 담당한다. 하위 컴포넌트는 자신의 형태만 다룰 뿐 자신이 어떻게 배치될 지에 대해 책임지지 않는데, 템플릿은 이러한 컴포넌트들을 어떻게 배치시키고 표현할 것인지 결정한다. 약간 UI = render(state) 느낌이기도 하다. 페이지 (Page)페이지는 템플릿에 예제/실제 데이터가 들어간 상태를 말한다. (ex) Mockup 템플릿에 데이터를 주고 초기화한 인스턴스라고 생각해도 좋다. 장점Atomic Design은 디자이너도 함께 실행해야 하는 것이지만 개발자 측면에서의 장점만 나열하겠다. 1. 원활한 의사소통컴포넌트 단위로 나누는 것은 그 단위가 미리 정의가 돼야 하며 모두가 동의해야 한다. 그러한 단위를 굳이 Atomic Design이 주장하는 바대로 따를 필요는 없지만 의사 소통 이전에 미리 정의돼야함은 변함이 없다. 이 때 Atomic Design은 이런 단위를 미리 정의해서 제공한다. 충분히 납득할만한 수준으로 잘 정의됐으며 이미 널리 알려져 있기 때문에 사용할 가치가 높다고 생각한다. 2. 의존성 분리Atomic Design에 부합하도록 각 요소를 잘 분리한다면 아래와 같은 실익을 얻을 수 있다. 개발 과정을 병렬로 진행할 수 있고 각 요소가 변경되더라도 변경되는 요소를 최소한으로 줄일 수 있음 3. 작은 컴포넌트Atomic Design의 각 단위에 맞게 하나의 역할을 하는 컴포넌트로 개발하면 각 컴포넌트를 작게 유지할 수 있다. 덕분에 그 개수가 늘어나도 변경에 의한 변경을 최소화할 수 있으며 따라서 복잡성이 지수적으로 올라가기보다 선형적으로 증가할 것이다. 뭐든 작게 유지하는 것은 key to computer science 이고 UNIX 철학에도 들어맞는다. Make each program do one thing well. 주의점1. 재사용을 위한 반응형 디자인컴포넌트는 데이터에 따라 여러 상태를 가질 수 있고, 따라서 여러 해상도에서 표시될 수 있다. 컴포넌트가 여러 해상도를 지원하려면 가로 너비(width)가 변할 수 있게 개발해야 한다. 123456789/* before */.button { width: 120px;}/* after */.button { width: auto;} 2. 하위 컴포넌트에서 레이아웃 속성 최대한 피하기Atomic Design에서 컴포넌트를 배치하는 역할은 상위 컴포넌트의 역할이다. 배치란 위치를 결정하는 일이고 따라서 각 컴포넌트는 자신이 그려지는 바깥 범위의 레이아웃을 방해하면 안 된다. 123456789/* 여백은 배치(레이아웃) 속성이며 이렇게 스스로 값을 줘버리면 배치하는 입장에서 굳이 덮어써야 한다. */.atom { margin-right: 30px;}/* Atom을 조합하는 Molecule이 조합된 Atom을 알아서 배치하는 것이 적절하다. */.molecule .atom { margin-right: 15px;} 3. CSS3 Flexbox/GridBox 사용하기flex, grid 속성은 배치에 최적화된 속성이며 Organism의 경우 List 컴포넌트와 같이 Molecule을 배치하는 역할을 주로 하기 때문에 이 때 사용하는 것은 적절하다고 할 수 있다. 1234567.organism { width: 300px; height: 300px; display: flex; align-items: center;} 실제 활용 예시Atomic Design으로 Todo 만들기 (KR) 출처: 더 괜찮은 웹 개발자가 되기 위한 리액트 스타일 가이드 (Aladin)","link":"/atomic-design-1/"},{"title":"10분 만에 기반 지식 없이 Node.js를 위한 Github Actions CI 구축하기","text":"목표 10분 만에 Github Actions를 통한 CI를 구축한다. master에 Merge 시에 ESLint와 테스트를 자동으로 수행하고, 결과에 따라 Merge할 수 없게 한다. 1. 기본 개념 설명기본 개념 몇 가지를 소개하고 넘어간다. 1. CIContinuous Integration. 각자의 코드를 병합하기 전에 검토하는 절차를 말한다. 보통 자동화된 상태를 지칭하며, master(or main) 등의 특정 브랜치에 Push(or Merge) Request가 올라오면 코드를 검토한다. 2. GitHub ActionsGitHub에서 특정 작업을 할 때 마다, 이벤트를 발생시키는데, 이를 구독해 특정 작업을 실행하는 것을 Github Actions라고 한다. (옵저버 패턴 참고) 3. Worflow FileGitHub Actions의 이벤트에 대해 무엇을 실행할 지에 대해 기록해 놓은 명령서를 workflow 파일이라고 한다. 4. Github Actions 사용 시의 CI 흐름PR Created(EVENT!) &gt; Build &gt; Test &gt; PR Merged(EVENT!) &gt; Deploy (배포 자동화는 다음에) PR을 생성할 때 CI 수행 Merge할 때 CD 수행 CI 과정에서 빌드가 성공했을 때만 Merge가 가능하게 설정하자. 2. Node.js App 으로 CI 구축 시작이 챕터에서 구축을 완료하고, 결과를 확인한다. 아주 간단한 과정이어서 CI라고 하긴 부끄럽지만, 아래 과정을 수행한다. npm module 설치 ESLint를 통한 코드 스타일 체크 테스트 실행 1. 필요한 자료 ESLint가 설치된, 스택에 상관 없는 Node.js 샘플 앱 샘플 앱을 올린 Public Repo가 필요하다. (이 글에서 코드를 따로 제공하지는 않는다.) 글쓴이는 토이 작업 중인 이 레포를 활용하였다. 글에서 Jest를 설치하고, ESLint와 연동할 것이다. 2. Node.js 템플릿 가져와서 사용하기아래는 레포지토리에서 Actions 탭을 눌러, Get started with GitHub Actions 아래에 있는 Node.js 템플릿을 가져온 것이다. 12345678910111213141516171819202122232425262728293031323334353637383940name: Node.js CI# 구독할 이벤트on: push: branches: [master] pull_request: branches: [master]# jobs 단위로 개별 서버(정확히는 Docker 컨테이너 단위라고 한다.)에서 작업이 수행된다.# 각 작업은 병렬로 실행 된다고 하는데, needs: build와 같이 표시해서 기다릴 수도 있다.jobs: build: # Ubuntu, Windows, MacOS를 지원한다. runs-on: ubuntu-latest # 영상에서도 소개됐는데, 변수 개념으로 생각하면 된다. # node-version 과 같이 배열로 돼있으면, 해당 원소를 순회하면서 작업이 반복해서 실행된다. # matrix 때문인지 배열만 되는 것 같다. (TODO) # 응용해서 runs-on에 여러 OS에서 돌릴 수도 있다. strategy: matrix: node-version: [14.x] # 템플릿 기본값: [10.x, 12.x, 14.x] # uses 개념은 다른 사람이 작성한 내용을 실행하는 개념이다. # actions/checkout: GitHub의 마지막 커밋으로 Checkout 한다. # actions/setup-node: Node.js를 설치한다. # run 개념은 명령어를 실행한다. 셸 스크립트와 동일하다. steps: - uses: actions/checkout@v2 - name: Use Node.js ${{ matrix.node-version }} uses: actions/setup-node@v1 with: node-version: ${{ matrix.node-version }} # npm ci는 npm install과 같은 기능을 수행한다. 자세한 내용은 아래 링크 참조. - run: npm ci # --if-present 옵션은 npm 스크립트가 존재할 때만 실행시키라는 의미이다. # 만약 build 스크립트가 없는 경우, 오류 없이 지나간다. - run: npm run build --if-present - run: npm test npm ci에 대한 스택 오버 플로우 설명 (속도가 2배 가량 빠르다고 한다. 캐싱에 대한 내용도 있으면 좋겠다) –if-present 옵션에 대한 스택 오버 플로우 설명 GitHub Actions Checkout ? 프로젝트에 이 파일을 .github/workflows/ci.yml로 저장한다. 이후 Push 하면 Actions 탭을 눌렀을 때 해당 빌드 과정이 수행됨을 볼 수 있다. 단, 아직 설정이 다 끝나지 않았으므로, 이 파일의 구조만 확인하기 바란다. 2. 기본 제공 Workflow에 Lint, Test 추가Lint와 Test 과정을 추가한다. 각 과정은 실패 없이 진행돼야 빌드가 성공한다. Lint 과정에선 error로 설정된 Rule을 위반한 경우 빌드가 실패하게 된다. 1. Lint: node_modules 에 있는 ESLint를 수행하는 스크립트가 필요하다. 아래 내용을 추가하자. 12345//...&quot;scripts&quot;: { //... &quot;lint&quot;: &quot;./node_modules/.bin/eslint .&quot;}, ci.yaml 파일에 npm run lint를 추가하자. 12345steps: # ... - run: npm run build --if-present - run: npm run lint - run: npm test 2. Test: Jest를 설치하고, 아래 내용을 추가하자. 폴더를 /tests로 설정했는데, 굳이 그럴 필요가 없다면 생략해도 된다. Test가 하나라도 실패하면 당연히 빌드는 실패하게 된다. 12345//...&quot;scripts&quot;: { //... &quot;test&quot;: &quot;./node_modules/.bin/jest --verbose ./tests&quot;}, Jest 폴더 설정 스택 오버플로우 설명 Jest는 글로벌로 API를 expose하기 때문에 ESLint error가 나지 않으려면 플러그인을 설치해줘야 한다. npm i --save-dev eslint-plugin-jest 로 ESLint-Plugin-Jest를 설치한다. eslintrc.yml 파일에 아래의 내용을 추가한다. 12345678910111213141516171819//...env: //... jest: true # Jest 글로벌plugins: - jest # Jest 테스트를 위해 플러그인이 필요하다.rules: //... # Jest Eslint 옵션은 0,1,2 (off, warn, error) 만 옵션으로 사용 가능하다. jest/no-disabled-tests: - warn jest/no-focused-tests: - error jest/no-identical-title: - error jest/prefer-to-have-length: - warn jest/valid-expect: - error ESLint 설정 스택오버플로우 설명 3. 빌드 성공 전에 Merge Button을 누를 수 없게 하기GitHub에서 Branch Protection Rule이라는 기능을 제공한다. 레포지토리 &gt; Settings 탭 &gt; Branches 탭 &gt; Branch protection rules 탭 &gt; Add Rule 버튼 클릭 후 아래와 같이 설정하였다. 4. 끝! ci.yml 파일을 Push 하자. Master에 Push하거나 Pull Request를 올리자 CI가 동작함을 확인하자. 글쓴이는 아래처럼 잘 동작함을 확인했다. 코드 베이스가 작고, 테스트가 사실상 전무하지만, 그래도 Node 설치부터 실행까지 20초밖에 걸리지 않는다는 점은 신기하고 인상적이다. Public 레포로 작업하면 좋은 성능의 CI를 무료로 사용할 수 있어 좋은 것 같다. TODO CD 과정도 구축하기. Docker 레지스트리 배포가 일반적인 듯하다. (쿠버네티스가 사용되는듯) Jobs에서 build 하나만으로 괜찮은 것 같긴 한데, 나누는 case는 뭐가 있을지 확인해보기 npm run build 명령어로 무엇을 실행할지 고민해보기. Node.js로 프로덕션 배포를 해 본 적이 없어서 뭐가 필요한지 아직 파악하지 못 했다. GitHub Actions에 대해 이론적으로 더 공부해보고, 할 수 있는 것들 더 많이 배우기 Git Hooks라는 개념도 있다고 한다. 로컬 수준에서도 프로세스를 자동화할 수 있는 것 같은데, 한 번 알아봐야겠다. 기타 내용 정리Why is it free?public은 무료, private은 사용량 만큼 낸다고 한다. 왜 무료일지 확인해봤는데, 출처에 따르면 Open Source 프로젝트 지원이라는 명목이다. We want every open source project to be productive and use best practices, so Actions is free for the 40 million developers on GitHub to use with public repositories. For private repositories, Actions offers simple, pay-as-you-go pricing. (…) Supported OS위에서 언급했듯, Ubuntu, Windows, MacOS 이다. Docker 컨테이너로 작동한다고 하며, 매 번 Fresh한 Docker Container가 제공된다고 한다. 계기가 된 Video참고한 유튜브 비디오. 간단하게 Github Actions이 뭔지 영상을 보기만 해도 파악이 가능하다. https://www.youtube.com/watch?v=R8_veQiYBjI&amp;ab_channel=TechWorldwithNana 글쓴이는 도커 기본 개념과 컨테이너 개념에 조금 익숙한 상태로 봐서 쉽다고 느꼈지만, 정말 아무것도 모른다면 조금 어려울 수도 있다. 영상에서도 언급했듯 Github Actions의 설정 파일은 Docker와 비슷하다.","link":"/ci-1-github-actions-nodejs/"},{"title":"[1 Month Docker] 1. Docker의 기본 컨셉과 Hello World","text":"Docker의 기본 컨셉을 다루고, 간단한 Hello World를 실습한다. build, share, run: build: (생략) share: DockerHub에서 이미지를 공유할 수 있다. run: 공유된 이미지를 통해 누구나 컨테이너를 실행할 수 있다. 이미지? 일단 Docker의 재사용 단위라고 생각하자. 도커 컨테이너? 애플리케이션을 담은 박스. 이 박스에는 기기명, IP 주소, 스토리지가 딸린, Docker에서 만들어낸 논리적인 가상 컴퓨터가 있다. 애플리케이션은 이 컴퓨터에서 실행된다. 박스 안의 애플리케이션은 박스 밖을 볼 수 없다. 이 박스는 여러 개가 동시에 실행될 수도 있다. 박스는 같은 실제 컴퓨터를 공유하면서 격리된 환경을 갖는다. 일관된 작업 방식: 아무리 애플리케이션이 복잡하더라도 Docker Image 단위로 Share, Run 만 하면 된다. 몇 개의, 어떤 컴포넌트, 설정 파일, 라이브러리를 사용하는지는 중요하지 않다. Portability: Docker가 있는 컴퓨터에선 명령어 하나로 곧바로 설치가 가능하다. 효율적인 자원 활용: 도커는 VM이 그렇듯, 여러 애플리케이션을 동시에 실행하는 것으로 컴퓨터 자원을 최대한 활용할 수 있다. 다만 VM보다 나은 점을 아래 표로 정리했다. 사용 자원 Docker VM Guest OS 사용 여부 No (커널 공유) Yes 가상화 리소스 비용 매우 낮음 (커널 공유) 독립적인 OS 수준 Gust OS Update 다운로드 Base Image 교체 수동 설치 아주 작은 앱 띄우기 Yes No 인수인계/배포 비용 A Dockerfile hours of installation 책에서는 Guest OS License 비용 문제에서도 차이가 난다고 언급했지만, Docker Image 형태로 쓴다고 해서 License 비용이 낮아지거나 사라지지는 않을 것 같다. 반대로 대수가 늘어나기 때문에 Open Source 기반으로 사용하지 않을까 생각이 든다. 네이티브 vs Docker vs KVM(VM 계열) 벤치마크 p.19 참고 주요 도커 명령어: 명령어 기능 docker container ls 실행 중인 컨테이너의 목록 표시 docker container ls –all 전체 컨테이너의 목록 표시 (종료된 것 포함) docker container run –detach {IMG} 컨테이너를 백그라운드로 실행 docker container run –publish 8088:80 {IMG} Host의 8088 포트로 Listen하여 컨테이너의 80포트로 전달 docker container inspect {ID} 컨테이너의 상세 정보를 JSON으로 출력 docker container stats {ID} 컨테이너가 사용하는 Host 자원 출력 docker container rm (–force) {ID} 컨테이너를 완전히 제거 (실행 중인 경우 force) docker container rm –force $(docker container ls –all – quiet) 모든 컨테이너를 강제 제거 종료된 컨테이너는 제거된 것이 아니어서 계속 용량을 차지하며, 아래 작업이 가능하다. 그대로 다시 실행 컨테이너 내의 App이 생성한 로그를 확인 파일을 Host에서 or Host로 복사 컨테이너의 네트워크: 기본적으로, 각 컨테이너는 Host 네트워크에 대해 격리된다. 컨테이너는 Host 내의 가상 사설망으로 구성된다. Docker는 Host의 네트워크 트래픽을 가로채 컨테이너로 보낼 수 있다. Docker가 컨테이너를 실행하는 방법: Docker Engine은 Docker Backend이다. Docker API(HTTP 기반의 REST API)를 제공한다. 이미지 재사용에 관한 기능은 직접 하고, 컨테이너는 containerd에 기반해 관리한다고 한다. containerd는 CNCF에 의해 관리되는 오픈소스 프로젝트이다. Docker CLI: Docker의 Frontend이다. Docker Engine과 소통하는 방법을 제공한다. 기타 정보: Docker는 가장 인기가 많은 컨테이너 플랫폼이지만, 다른 기술도 있으며 컨테이너 기술로 인해 플랫폼에 락인될 걱정은 하지 않아도 된다. Docker는 이미지를 사용해 컨테이너를 실행한다. 이 때 이미지가 로컬에 있어야 한다. docker container run을 할 때에 없으면 docker pull을 받게 된다. 한 번 다운로드한 이미지는 재사용한다. 도커 컨테이너 Id는 컨테이너의 hostname이 된다. 컨테이너를 선택할 때, 이름 앞 몇글자만 입력해도 된다. 예: f1695...일 때, docker container top f1만 해도 된다. 실습솔루션 목표: 실행 중인 Apache 컨테이너에서 index.html을 변경하라. 힌트: 컨테이너는 독립된 파일 시스템을 가지며, 컨테이너 내의 웹 서버 또한 컨테이너의 파일 시스템의 파일을 제공한다. docker container 명령어를 통해 컨테이너에서 수행할 수 있는 명령어 목록을 볼 수 있다. docker {command} --help를 통해 해당 명령어의 상세 설명을 확인할 수 있다. diamol/ch02-hello-diamol-web 이미지는 /usr/local/apache2/htdocs 폴더 내의 파일을 정적으로 제공한다. (윈도우의 경우, C:\\user\\local\\apache2\\htdocs 폴더.) 내 풀이풀이 과정을 서술함. 1. 제공된 컨테이너 트러블 슈팅: 일단 ch02-hello-diamol-web 의 기본 포트인 8088은 접속할 수가 없었다. 그래서 DockerHub 가서 Apache 이미지를 받아서 실행해봤다. 8080 포트로 잘 되더라. 이 때 명령어가 $ docker run -dit --name my-apache-app -p 8080:80 -v &quot;$PWD&quot;:/usr/local/apache2/htdocs/ httpd:2.4 였는데, 배운 점: -dit: --detach --interactive의 약자인데, -dit가 필요한 이유를 보면, bash 스크립트가 엔트리 포인트인 경우 -d만 하면 정지된 상태에서 아무것도 못한다고 한다. -it를 줘서 셸이 있어야 스크립트가 실행된다고 한다. -p: --publish의 약자이다. -v: 아직 안 배웠지만, 볼륨 개념일 것으로 추정된다. 도커 자체의 네트워크 문제가 아님을 알고, 80으로 하니까 잘 됐는데, 이유는 모르겠다. 2. 컨테이너 셸 접속: 일단 docker container exec -it --tty {id} /bin/bash 로 접속할 순 있었다. (나오는건 exit 치면 된다.) --tty 옵션에 대한 글 참고 3. 직접 파일 수정: 무슨 망할 기반 이미지를 쓰는지 vi 밖에 지원을 하지 않아서 직접 수정은 포기했다. 파일을 복사해야 하는데, 어떻게 하는지 모르겠다. 4. 파일 복사 방법: Dockerfile을 수정하는 게 가장 쉬울 것 같았지만, 제공되지 않아서 할 수 없었다. 복사를 해야 하는데, 호스트에서 컨테이너로 파일 복사하기로 docker cp 명령어를 배워서 수행했고, 성공했다. 매우 작은 작업이었지만 너무 오랜 기간이 걸렸다. 아무래도 기록하면서 하니까 오래 걸리고, 책의 내용을 요약했음에도 불구하고 며칠만에 다시 보는거여서 오래 걸렸다. 많이 헤맨 덕분에, docker container ls, docker container rm, docker container exec, docker container run은 정말 많이 사용해서 다행이다. 참고 자료: Docker In A Month of Lunches (Manning, 2020) 추가로 읽을 것: Docker와 VM","link":"/docker-diamol-1-hello-world/"},{"title":"[1 Month Docker] 2. Dockerfile, Docker Image","text":"Dockerfile과 Docker Image 개념을 소개하고, 핵심적인 내용을 설명한다. Docker에 대한 간단한 소개의 내용을 기본으로 가정하고 시작한다. 저번 글에선 Container와 Docker를 체험해보았다. Container는 어떠한 스택의 애플리케이션이든 배포 측면에서 일관된 경험을 제공하므로 사용하는 것이 좋지 않을까 생각한다. Docker로 Container를 실행하려면 Docker Image가 필요한데, 이번 글에서는 최종적으로 Image를 직접 생성한다(공식적으로는 build 한다고 표현함.). 1. 기초 개념 설명1. Dockerfile: 이미지 빌드 명령어의 입력으로 들어가는 스크립트이다. 아래와 같은 내용을 담는다. 12345678910111213141516171819202122# Parent Image 지정# Dockerfile은 이미지를 정의하는 파일이다.# 새 이미지를 만들 때 다른 이미지의 내용에 기반해 덧씌우는 형태이다.FROM diamol/node# 환경 변수 3개 설정# Docker와 같이 컨테이너 환경으로 앱이 배포되는 경우,# 환경 변수를 arguments로 많이 활용한다.ENV TARGET=&quot;blog.sixeyed.com&quot;ENV METHOD=&quot;HEAD&quot;ENV INTERVAL=&quot;3000&quot;# Working Directory를 /web-ping으로 지정 (폴더 생성 후 이동함. mkdir &amp;&amp; cd)WORKDIR /web-ping# Host의 app.js 파일을, Working Directory(.)에 복사COPY app.js .# node로 다음의 js를 실행# CMD 명령어는 컨테이너 실행 시에 1회 수행되는# container.once('start', callback)과 같다.CMD [&quot;node&quot;, &quot;/web-ping/app.js&quot;] 아마 셸 스크립트에 익숙한 사람은 셸 스크립트와 다름 없다고 생각할 것이다. 맞다. 똑같다. 아마 셸 스크립트가 익숙하지 않으면 Dockerfile에 쉽게 친해질 순 없을텐데, 리눅스 환경 구성 기초 | T 아카데미나 리눅스 커맨드 라인 &amp; 쉘 스크립트 #1 | ABCD DevOps라는 좋은 자료가 있으니 참고하자. Dockerfile 안에서만 쓸 수 있는, Dockerfile에서 쓰일 만한, 명령어가 10개 정의돼있다. 이 명령어들이 주축이 돼서 Dockerfile의 내용을 구성하게 된다. 전체 기능에 대해서는 Dockerfile Cheat Sheet를 참고하라. 걔 중 유사한 명령어인 CMD vs RUN vs ENTRYPOINT를 정리한 글도 있으니 참고하기 바란다. Dockerfile 명령어는 대소문자를 구분하지 않지만 대문자로 쓰는 게 컨벤션이다. 2. Image: 이미지는 Dockerfile에서 기술한 내용이 실행된 모습을 스냅샷 형태로 담은 파일이다. 컨테이너 실행 시 이미지를 통해 Dockerfile에 정의된 내용이 그대로 재현된다. 3. Image 받아오기: 이미지를 직접 생성하지 않고, DockerHub 등의 Docker Registry (이미지 저장 서버)에서 받아올 수도 있다. 단순히 받아오기만 하는 명령어는 docker image pull 이다. docker image pull diamol/ch03-web-ping 을 실행해 DockerHub에서 이미지를 받자. 하나의 이미지를 받는데, 여러 Pull Complete가 표시돼있다. (나중에 설명한다.) 4. Image 빌드: docker image build 명령어를 실행하면, 이미지는 자동으로 빌드된다. 예: docker image build --tag web-ping . =&gt; web-ping이라는 이미지를 생성. (Mandatory) .은 Dockerfile 및 COPY 등에서 Host의 기준 디렉토리로 사용된다. (Mandatory) --tag는 이미지의 이름을 지정한다. 주의: 파일을 Windows -&gt; Linux로 복사하는 경우, 권한이 rwxrwx로 지정되는데, 이는 서로 권한 정보가 호환되지 않기 때문이다. 로컬에서 직접 빌드된 이미지는 도커 엔진에 캐시돼 보관된다. 새로운 버전을 빌드하려는 경우, --tag web-ping:v2와 같이 :으로 버전을 구분하여 명시하면 된다. 5. Image 실행(컨테이너로): docker container run {image_name}으로 실행 6. Image Layer: 이미지에는 생성 과정에 대한 메타데이터도 포함된다. 이미지 생성 과정을 통해 docker image history web-ping Docker Image는 Image Layer라는 더 작은 개념으로 구성되며, Dockerfile의 각 명령(CREATED BY) 마다 Layer가 생성된다. 이미지는 각 Layer의 논리적인 집합이다. Layer는 도커 엔진에 물리적인 파일의 형태로 캐시되는 단위이다. 이미지 간에 Layer가 공유되므로 전체 용량 부하를 낮출 수 있다. docker image ls로 논리적인 용량을 확인할 수 있지만, docker system df로 이미지가 차지하는 물리적인 용량을 확인할 수 있다. 이런 Image Layer 캐시를 활용하려면 조건이 필요한데: Layer 이전의 Layer 들의 내용과 순서가 바뀌지 않아야 한다. 이전 내용이 바뀌었는데, 이 명령(Layer)을 실행한 결과가 같음을 보장할 수 없다. 만약 내용을 바꾸는 경우, 이 Layer에 의존하고 있던 모든 이미지에 영향을 끼친다. 그러므로, 이전 Layer가 변경되는 경우, 이후 Layer는 캐시로 사용될 수 없게 되고, 새로 Layer를 생성하게 된다. 7. Layer 캐시 최적화 전략: Layer 캐시 활용을 통해 전체 용량과 이미지 빌드 시간을 줄일 수 있다. 이미지에서 변하지 않는 부분을 최대한 먼저 실행해 새로 빌드할 Layer 수를 줄인다. 캐시 사용 가능 여부는 Instruction의 내용과 Arguments(명령어 내용일 수도 있지만, COPY와 같은 경우 파일의 내용까지.)로 Hash 값을 만들고 비교하여 결정한다. Hash가 일치하는 경우 빌드하지 않고 도커 엔진에 캐시된 Layer를 사용한다. 일치하지 않는 경우, 해당 Layer부터 최종 Layer까지 새로 빌드한다. (뒷 Layer의 해시가 같아도, 재사용할 수 없다.) app.js 파일을 수정한 후 (nano app.js) 빌드한 모습이다. COPY app.js를 수행하는 step 6가 다시 Layer를 만듦을 확인할 수 있고, 이후 Layer인 step 7은 바뀐 내용이 없지만 앞 Layer가 바뀌어서 다시 만들어짐을 확인할 수 있다. 8. Layer 캐시 최적화 예시: 123456789101112131415FROM diamol/node# 시작 시 실행될 명령어를 지정하는 것이므로, 어디에 놓아도 상관 없다.# 캐시를 위해 앞에 놓는다.CMD [&quot;node&quot;, &quot;/web-ping/app.js&quot;]# 환경 변수 3개를 한 번에 등록해 Layer 개수를 줄였다.# 개수를 줄인 것과 캐시 최적화는 큰 연관은 없지만...ENV TARGET=&quot;blog.sixeyed.com&quot; \\ METHOD=&quot;HEAD&quot; \\ INTERVAL=&quot;3000&quot;WORKDIR /web-pingCOPY app.js . 이제 docker image build -t web-ping:v3를 실행해보자. 환경 변수 개수가 줄어들어 7단계에서 5단계로 줄었음을 확인할 수 있다. 이제부턴 app.js를 수정해도 마지막 Layer만 바뀐다. 2. 실습1. 목표: diamol/ch03-lab 폴더의 이미지에서 /diamol/ch03.txt 파일을 수정하고 새 Image를 생성하라. 이 때 Dockerfile을 수정해서는 안 된다. 2. 힌트: -it으로 컨테이너에 키보드 I/O 가능 컨테이너 파일 시스템이 Exit 상태에도 제거되지 않음을 활용 docker container --help로 모르는 명령어에 대해 공부할 것 3. 처음 생각한 접근 방법: Container에서 일단 파일을 수정한다. 컨테이너로 이미지를 생성해낸다. 명령어를 찾아보자. 4. 실제 수행 과정: 1. 일단 이미지를 빌드함 cd ../../lab (빌드를 위해 lab 폴더로 이동) docker build image -t ch03-lab . (빌드 성공) 2. 이제 컨테이너를 실행해야 함 docker container run ch03-lab (실패) docker container ls (없었음) cat Dockerfile (CMD 등 명령어 실행이 없고, COPY 뿐이었음) 3. 컨테이너에서 수행할 명령어로 주어 실행해야 함 docker container run ch03-lab /bin/bash (실패) docker container run ch03-lab /bin/sh (이미지에 bash가 없었음..) vi ch03.txt (텍스트 파일 수정) exit (sh 나옴) 4. 정지된 컨테이너를 이미지로 빌드해야 함 Docker Commit Reference를 참고해서 빌드 명령어 학습 docker container ls --all 로 종료된 컨테이너 ID 확인 (67a) docker image commit 67a ch03-lab:v2 (무슨 해시값이 출력됨..) docker image ls (v2로 생성됨을 확인) docker container run ch03-lab:v2 cat ch03.txt (파일 갱신됨을 확인) 끝! Lab 하면서 배운 점: docker commit 명령어로 컨테이너 내용으로 이미지를 빌드할 수 있다는 점. 다만 이렇게 되면 Dockerfile은 없는게 아닌가? docker container run {IMAGE} {COMMAND}로 명령어를 실행할 수 있음 다만 이는 이미지에서 수행하는 명령어가 없는 경우에 한한 것 같고, docker container exec으로 셸을 띄우는 것이 일반적인 것 같다. TO DO: 컨테이너에서 Commit으로 생성한 이미지에서 Dockerfile을 추출할 수 있을지 확인해보기","link":"/docker-diamol-2-dockerfile-image/"},{"title":"Express를 사용해야 할 이유 (2) - Microframework","text":"이 글은 microframework라는 개념과 express의 용도를 연관지어 생각해본다. 이 글은 기술적으로 사실이라고 검증되지 않은 내용이 포함되며 데이터에 근거한 결론보다 생각 위주로 작성됨을 미리 알린다. 대략 2주 전 Express를 사용해야 할 이유 (1)을 쓰면서 왜 Express를 많이 쓰는걸까? 생각을 많이 해봤지만 결론을 내리지 못 했다. 아래는 당시 글에 작성했던 intro이다. 잠시나마 사용해본 Express는 내게 React 같았다. 무엇이든 할 수 있어 보였으나 직접 하기에는 매우 불편하고, 그러다보니 REST API를 작성할 때 이런 것까지 해야 돼? 혹은 이런 기능이 없어서 불편하네 등이 많았는데… 오늘이 되어서야 왜 Express가 기능이 적은지 알게 되었는데, 그 마법의 키워드는 바로 Microframework이다. (진지하게 이 키워드에 대해 오늘 이전에 들어본 적이 단 한 번도 없었다.) Microframework란?1. 정의: 최소 기능을 갖는 웹 애플리케이션 프레임워크 &lt;-&gt; full-fledged framework (필요한 기능은 대부분 갖춘 프레임워크를 의미) 2. 기능: Microframework는 서비스 개발 시 “주로“, “일반적으로“ 사용되는 공통적인 기능들을 제공하지 않는다. 제공하지 않는다는 그 기능들이란 대체로 아래와 같다. 인증, 인가 ORM 혹은 DB 관련 기능 입력값 검증 / 보안 (Validation, Sanitation) 템플릿 엔진 3. 목적: Microframework는 작은 API 서버를 제작하는게 목적이다. 정리하자면 Microframework는 기능의 다양성이나 설계의 편리성보다 기능의 단순성이 더 우선된 프레임워크이다. 따라서 규모 있게 Monolith로 제작하는 경우 full-fledged framework를 사용하는 게 맞다고 본다. 실제로 Walamrt는 Node.js 기반으로 백엔드를 구성했지만 Commerce 기업이라 Hapi.js(full-fledged framework)를 직접 만들어서 사용하기도 하고 말이다. Microframework의 종류아래는 주요 언어의 Microframework의 목록의 일부다. Express.js for Node.js Flask for Python Sinatra for Ruby Spark for Java (NOT Apache Spark) (더 많은 목록은 위키 백과 Microframework 문서 (EN)를 참고 바람.) 예상한 대로 Express.js도 microframework로 등재돼있다. Express 소개문에 “Fast, unopinionated, minimalist web framework for Node.js“ 라고 괜히 되어 있는 것이 아니다. Netflix에서는 Restify라는 프레임워크를 예전부터 사용 중인데, Restify도 Microframework이다. Restify는 Semantically correct RESTful 을 지향한다. 온전히 REST API를 위한 기능만 제공하므로 Express 등에서 제공하는 템플릿 엔진 조차 없는데 기능의 단순함 측면에서 더 매력적이라고 할 수 있다. Restify는 또한 Connect 미들웨어를 지원하므로 Express 미들웨어와 호환된다. 아래는 Restify 공식 홈페이지에 나와있는 소개이다. Meet Restify - A Node.js web service framework optimized for building semantically correct RESTful web services ready for production use at scale. restify optimizes for introspection and performance, and is used in some of the largest Node.js deployments on Earth. Microframework가 Microservices에 좋을까? 아직 MSA에 대해 공부한 적 없기도 하고 이런 지식들이 쉬운 편이 아니라 좀 배워야 키워드 검색이 가능하기 때문에 Youtube와 여러 글들을 읽어보면서 느낀 점을 적으려 한다. 왜 restify를 사용하는지 추측해보자면 굉장히 많은 컨테이너에서 돌아가는 MSA를 구축할 때 작은 서비스가 유리하기 때문이 아닐까? Netflix는 (다른 기업들이 응당 그러하듯) MSA로 개발할 때 기능 단위로 분리하는데, Monolithic Service에 들어갈 많은 기능들이 기본 제공되지 않는 프레임워크가 가벼워지는데 유리한 것이다. (Netflix는 의사 결정 시 성능에 우선순위를 두는 듯 하다. 13년도에 Java에서 Node.js로의 이주를 시작한 것만 봐도.) Node.js를 쓰는 대부분의 use case가 microframework가 필요해서일까?이전 글 내용 중 다운로드 수를 비교한 자료가 있었는데 Express가 압도적이었다. 같은 microframework이며 벤치마크(hello world)도 더 우수한 Koa가 그렇게 많이 사용되지 않는 점(약 21배 차이)은 이전 글에서도 다뤘듯 async/await 문법의 지원과 커뮤니티의 차이 때문임으로 보인다. 다운로드 수에 대한 요즘 생각: 러닝 커브. 많은 사람들이 Javascript가 배우기 쉽다고 말하며 그렇게 진입하는 사람이 적지 않다. 그저 React.js와 Express.js를 기초적인 수준에서 사용하는데 머무는 사람들이 정말 많다고 생각한다. Netflix처럼 애초에 Managed로 환경 구성이 잘 된 경우라면 npm에서 다운로드 수 집계가 제대로 되지 않을 것 같다. (Proxy를 통한 캐싱 등) 즉 npm 다운로드가 현업 개발 시의 실제 사용 빈도를 정확히 나타내는 것도 아닐 수 있을 것이다. 따라서 단순히 npm 다운로드 수로 비교하는 것보단 실제로 돈을 벌고 많은 트래픽을 처리하는 기업에서 무슨 스택을 사용하는지가 더 중요할 것으로 보인다. Hapi Koa Nest Express (참고로 restify는 12만, fastify는 19만 정도 된다. - fastify의 경우 Nest.js에서 사용되는 면이 있으니 참고) 다운로드 수를 유일한 척도로 삼고 맹신해서는 안 될 것 같다.Express의 점유율이 매우 높다는 것을 microframework를 사용하는 숫자가 Node.js 백엔드 개발자 중에서 대다수라고 받아들이면 안 될 듯하다. Netflix의 경우는 MSA를 도입했기 때문이지만 트래픽이 많지 않은 기업들의 경우 아직 Monolithic이거나 작은 규모의 분산 시스템이면 충분할 거라고 생각하기 때문이다. Node.js의 경우 … Javascript를 프론트엔드와의 공용어로 사용할 수 있다는 점 SSR 시 코드 재사용이 가능하다는 점 백엔드 언어와 Javascript 간의 Context Switching이 사라진다는 점 Interpreter 언어여서 Startup이 매우 빠르다는 점 모듈 생태계가 크다는 점이 장점 이런 장점 속에서 굳이 프레임워크에서 가치를 찾지 않으려는 경우도 많지 않을까? 다운로드 수가 많다고 해서 Express를 사용할 이유는 없다. 1편에서도 밝혔듯이 Javascript와 Node.js의 장점 자체도 이미 많으며 Express에서 조금 고생하면서 Monolithic 서비스 개발하는 것은 점진적으로 러닝 커브가 올라가는 형태라고 생각하고, 딱 그 정도 수준이 필요한 기업도 많을 거라고 생각한다. 아직 결론은 내릴 수 없을 것 같다.아직 Node.js의 정수를 다 배우지 못 했기 때문에, 아직 MSA를 배우지 못 했기 때문에 정확한 판단을 내릴 수가 없다. 이것들을 어느 정도 습득하고 나서 정말 왜 Express가, Express만이 이렇게 잘 팔리는 이유를 분석할 수 있으면 좋겠다. What I Learned Netflix는 기술 블로그와 컨퍼런스를 통해 정말 많은 기술적인 내용들을 공유한다는 걸 오늘 리서치하면서 배웠다. Youtube의 경우 여러 채널에 Video가 산재돼있는데 이 Playlist가 좋은 것 같다. 아직도 Node.js 생태계에 대해 제대로 이해하지 못 하고 있다는 점을 또 알게됐다. 아직 해결되지 못한 질문들에 대해 데이터를 찾아서 반드시 답을 내리고 싶다.","link":"/ejs-1-why-express-2/"},{"title":"Express를 사용해야 할 이유 (1) - 생태계 조사","text":"잠시나마 사용해본 Express는 내게 React 같았다. 무엇이든 할 수 있어 보였으나 직접 하기에는 매우 불편하고, 그러다보니 REST API를 작성할 때 이런 것까지 해야 돼? 혹은 이런 기능이 없어서 불편하네 등이 많았는데 이번에 알아보려고 한다. 얼마나 많이 Express를 사용하며, 왜 Express를 사용하는지 팩트 위주로 체크해봤다. 1. Node와 Express의 장점을 헷갈리면 안 된다.대부분의 웹사이트에서 소개하는 Express의 장점들은 Javascript, Node.js의 장점들이었다. 많은 글을 읽어보아도 Express의 장점을 소개하는 글은 많이 없었고 대부분 Node.js의 장점을 소개하고 있었다. Express가 Node 기반인 게 큰 장점이라는 걸까… 그래서 Node.js와 같은 목적으로 생성된 프레임워크/런타임을 조사해보았다. 2. Reactor Pattern을 구현한 프레임워크/런타임A. 역시 Node.js만 있는 것은 아니었다. Javascript를 깊게 배우고 생태계를 옮겨 탈 바에 기존에 사용하던 언어로 작업하는 게 현실적이긴하다. Lang Sync Framework Async Framework Java Spring Web MVC Spring WebFlux (Reactor Pattern), Vert.x (JVM 기반) Python Flask, Django FastAPI, Tornado, Sanic, … (꽤 많다.) Javascript - * 다른 언어에 대해선 찾아보지 않았지만 Java, Python이 점유율이 큰 언어들이므로 충분하다고 생각한다. 벤치마크를 찾아보진 않았지만 같은 패턴을 기반으로 제작됐기 때문에 실제 서비스로 구현했을 땐 성능 면에서도 비슷할 것으로 예상된다. 다만 Node.js의 장점이라면, 선천적으로 비동기 API가 장려되어왔기 때문에 비동기 API로 작성된 라이브러리 활용 면에서 낫지 않을까 생각한다. 출처: 대용량 트래픽을 감당하기 위한 Spring Webflux 도입, Spring Webflux는 어떻게 적은 리소스로 많은 트래픽을 감당할까 3. Node.js 백엔드 프레임워크 간의 점유율/만족도 비교제대로 비교하기 전에 통계 자료부터 확인하자. 참고로 Next.js는 SSR 용 백엔드(SSR, Code Splitting 자동)라고 생각하면 된다. 점유율 요약 (아래 그림): Express의 점유율이 압도적이다. Koa, Hapi 라는 네임드의 점유율이 꽤 낮다. 서비스 개발에 가장 유리할 거라고 생각했던 Nest.js의 점유율이 13%밖에 안돼서 의문이다. Hapi Koa Nest Express 물론 Express를 기반으로 하는 다른 프레임워크 등이 어느 정도 반영됐을 것이긴 하다. Nest도 처음에는 Express 기반이었으니까. 그래도 다운로드 수의 큰 차이를 보면 나머지 프레임워크의 시장성이 의심되긴 한다. 만족도 비교 (아래 그림): Express는 점유율에 이어 만족도도 최상위권이다. Nest.js가 5% point 정도의 차이가 있지만 준수한 편이다. Koa의 만족도가 76%인 점인 이유는 장점이었던 동기식 코딩 방식인 async-await이 표준화됐기 때문임으로 보인다. Hapi는 만족도가 매우 낮은 것으로 보아 사용할 수 없겠다는 생각이 들었다. (추후 조사를 해봐야겠다.) 출처: 2020 State Of JS (한국어 번역) 4. 왜 이렇게 Express를 많이 쓰는 걸까? 정말 Express가 좋은걸까? 다른 언어의 프레임워크를 비교해봤을 때 솔직히 좋다고 하진 못 할것 같다. 1. 단순함 (+0)정말 많은 블로그에서 Express의 최장점을 단순함으로 꼽고 있었는데 장점보다는 목적에 가까운 것이라 생각한다. 목표에 따라 단순함은 장점이 될 수도, 단점이 될 수도 있기 때문이다. 단순함을 장점으로 꼽는 경우 둘 중 하나이다. Rich Framework를 감당할 만큼 숙련된 개발자로 채우기 어려운 조직이거나 애초에 큰 규모의 서비스를 작성하기 위해 Express를 사용하지 않거나 만약 서비스 개발을 위해 Express를 사용한다면 단순함은 직접적인 단점이 된다. 기본적인 의존성만 담은 Boilerplate(1.4k stars)만 보더라도 같이 깔아야 할 라이브러리들이 많아 학습 곡선이 가팔라진다. 처음 입문하는 경우 미들웨어들을 직접 찾는 추가적인 일을 하게 된다. (한 프레임워크 내에서 찾는 것과 대조적.) Rich Framework 들에 비해 설계를 너무 근본적인 것들부터 해야 해 오히려 설계 측면에선 난도가 높다. (DI/IOC가 없고 여러 라이브러리를 비교 분석 후 사용해야 함.) 다만 적절한 Boilerplate를 찾으면 이 문제가 어느 정도 해소된다는 점과 이후 단락에서 소개할 내용들을 통해 단순함의 단점을 상쇄할 수 있다. Node.js는 출시 후 아직까지도 작은 서비스를 만드는 데 적합하다는, 프로토 타이핑 위주라는 인식이 남아 있는 것 같고, 그런 용도로 채택하여 단순함이 종종 장점이 되는 것 같기도 하다. 2. Express Middleware (+0)어떤 언어, 프레임워크로 웹 개발을 하더라도 Express에 미들웨어에 해당하는 계층에서 확장성을 가져가는 것은 기본이지 특별한 기능은 아니다. 또한 Express에서 제공하던 자체 Middleware들은 모두 Connect 미들웨어 라이브러리로 옮겨갔다. Next.js에서는 이 미들웨어들을 지원하는데, 그럼 다른 프레임워크에서도 의도하기만 하면 재사용 할 수 있는 셈이다. (의존성이 req, res, next 인자 밖에 없으니.) 3. Community (+3)Express는 꽤 많은 사용자 풀을 보유하고 있다. 이미 사용자가 많아 검색을 통한 문제 해결이 비교적 원활하다. 아래는 StackOverFlow 트렌드인데 koa(js), hapi(js)는 태그로 잡히지도 않아서 비교가 불가능했다. 이는 생태계 조성이 거의 전무하다는 뜻인데 koa나 hapi는 출시된 지 시간이 지났음에도 이정도이며 특히 Hapi는 정말 작은 사용자 풀을 보여준다(사용하지 마세요). 4. Async-await을 workaround로 쓸 수 있다. (+1)Express v5 부터는 Response Handler 및 Middleware에서 async/await을 사용할 수 있지만 아직 Release 되지 않은 관계로 사용할 수는 없다. Express QnA 이슈의 답변이다. Q. How to use async/await in express 5? A: There is one main difference between v4 and v5 when it comes to async/await and promises in general. In v5, if you return a promise from a response handler (or middleware), if that promise rejects and is not handled elsewhere, then Express will handle the error. It handles the rejection by passing the rejection reason to next for you. v4에서도 async-await을 쓸 수 있는데, 아주 간단한 미들웨어 express-async-handler로 한 번 감싸주면 된다. (원리는 이 설명 참고) (같은 원리로 Promise도 처리 가능) 12345678910111213// find a user by idrouter.get( '/:id', asyncHandler(async (req, res) =&gt; { if (req.user.id === req.params.id) { return res.status(403).send(FORBIDDEN); } const user = await UserRepository.findUserById(req.params.id); if (!user) return res.status(404).send(NOT_FOUND); res.json(user); }),); 5. Koa나 Express나 둘 다 개발은 하지 않는다. (+0)Koa나 Express나 발전을 멈춘지 좀 됐다. Koa의 Roadmap 3.0을 보면 현재 모습이 Koa의 완성형이라고 생각할 수 있다. 17년도 이후로 유지보수가 대부분 Documentation에 치중되어 있다. KoaJS는 2013년에 시작해 제너레이터 기반으로 미들웨어를 쉽게 작성하기 위해 나온 프레임워크인데, async-await 표준이 2017년 초부터 Node.js에서 공식적으로 지원되면서 그 의미가 퇴색되지 않았나 생각이 든다. Express 역시 Documentation 위주의 유지보수, v5를 6-7년 째 안 내고 있긴 하다. (14, 15년도 쯤까지만 일한듯) Is express dying? Release v5 Router 모듈 (Express가 라우팅 관련 책임을 이 모듈로 넘긴 듯하다.) (얘도 유지보수가 죽었고.) Node.js Foundation to Add Express to its Incubator Program 16년 초에 Express 소유권이 넘어갔다고 한다.(인과관계는 잘x) 5. 서비스 개발 측면에선 NestJs가 더 낫지 않을까? (추후 보강 예정)Express, Koa는 현재 사실상 유지보수가 되고 있지 않다. 프로젝트에서 돈을 벌지 못하기 때문인 것으로 보이는데, 기업 스폰서가 없으며 프레임워크도 간단해 기술 지원이 불가능해 수익 모델이 없다. (Hapi는 Walmart에서 사용 중이긴 하지만 너무 마이너하다. 왜 인기가 없을까?) NestJS는 구조가 Angular의 영향을 받았다고 돼있지만 Spring과 유사한 구조와 개발자 경험을 제공한다고 생각하며, Spring은 그 기능과 복잡성을 통해 기술 지원으로 돈을 벌고 있기 때문에 NestJS가 이 모델을 구현한다면 긴 시간 유지보수를 해나갈 수 있을 것 같다. 정확히 무슨 벤치마크를 했는진 모르겠지만 성능 측면에서 NestJs-Fasitfy[현재 버전]가 Express보다 낫다고 한다. (출처) Framework Req/sec Trans/sec Req/sec DIFF Trans/sec DIFF Nest-Express 15370 3.17MB +4.38% +4.23% Nest-Fastify 30001 4.38MB +2.20% +2.23% Express 17208 3.53MB +8.38% +8.31% Fastify 33578 4.87MB +6.55% +6.53% NestJS에 대해선 추후 더 조사하려고 한다. TODO1. NestJSNestJS는 다루는 양이 방대하기도 하고 앞의 리서치에서 시간을 너무 많이 사용해서 따로 시간을 내서 리서치하진 못 해서 다음 기회에 꼭 하도록 한다. 2. Fasify한 번 조사해봐야 할 것 같다. async-await도 지원하며 제대로 관리되고 있는 것 같다. 아래는 README에 게시된 벤치마크인데 성능도 역시 좋고. Framework Version Router? Requests/sec Express 4.17.1 ✓ 15,978 hapi 19.1.0 ✓ 45,815 Restify 8.5.1 ✓ 49,279 Koa 2.13.0 ✗ 54,848 Fastify 3.0.0 ✓ 78,956 - http.Server 12.18.2 ✗ 70,380 3. HapiJSHapi도 개발이 계속 진행 중이고 Nest처럼 Rich한 Framework를 목표로 하는 것 같고, Walmart에서 실제로 사용하면서 주도적으로 개발하다가 작년 중순부터 Community-driven으로 간다고 한다. 성장 가능성이 꽤 있는 것 같아서 시간이 나면 조사하면 좋을 것 같다. Facebook이 React를 만들어 프론트엔드 생태계를 많이 바꿔낸 것처럼. 4. Express In Action (2016)이 책을 좀 더 읽어보고 Express의 가치를 발견하다면 정말 좋을 것 같다. 5. 기타Promise, Async-await이 성능이 CPS 패턴에 비해 느리다는 의견이 종종 나왔는데 왜 그런지 확인해보기 D2에서 Node.js는 Socket.IO 때문에 떴다고 하던데 정말인지 확인해보기","link":"/ejs-1-why-express/"},{"title":"실행 결과를 재사용하는 함수 skip","text":"사용 예시12345&lt;ul id=&quot;list&quot;&gt; &lt;li&gt;Item 1&lt;button&gt;삭제&lt;/button&gt;&lt;/li&gt; &lt;li&gt;Item 2&lt;button&gt;삭제&lt;/button&gt;&lt;/li&gt; &lt;li&gt;Item 3&lt;button&gt;삭제&lt;/button&gt;&lt;/li&gt;&lt;/ul&gt; 1234567891011121314151617const askOnRemove = skip(() =&gt; confirm( '정말 삭제하시겠습니까? 최초 1회만 확인합니다.', ),);const list = document.getElementById('list');list.addEventListener('click', (e) =&gt; { const { nodeName, parentNode: targetItem, } = e.target; const { parentNode } = targetItem; nodeName === 'BUTTON' &amp;&amp; askOnRemove() &amp;&amp; parentNode.removeChild(targetItem);}); 함수 본문123456function skip(callback) { let result; // 클로저 영역 return function (...args) { return result || (result = callback(...args)); };} 함수 본문 해설함수 skip은 callback를 받아 실행하고, 만약 그 함수의 반환값이 truthy이면 해당 값을 반환하고, 이후에는 함수를 실행하지 않는다. 함수형 프로그래밍과의 연관성 함수형 자바스크립트는 함수를 N 단계로 조합해서 사용한다. 즉, 고차 함수 응용의 반복이다. skip은 고차 함수이고, callback은 skip이 남겨 놓은 로직을 완성하는 함수다. 예시로 사용된 askOnRemove는 클로저인데, 함수로 만들어진 함수는 대부분 클로저다. 고차 함수란?함수를 인수로 사용하거나 함수를 반환하는 함수이다. 예시: const filter = (predicate, xs) =&gt; xs.filter(predicate) // 함수가 인자 const is = (type) =&gt; (x) =&gt; Object(x) instanceof type // 함수를 반환 filter(is(Number), [0, '1', 2, null]) // [0, 2]","link":"/fjs-1-skip/"},{"title":"함수를 부분 실행하는 currying 개념","text":"사용 예시 112345678&lt;Field name=&quot;username&quot; onChange={handleChange('username')}/&gt;&lt;Field name=&quot;password&quot; onChange={handleChange('password')}/&gt; 함수 본문 1123const handleChange = (fieldName) =&gt; (value) =&gt; { fields[fieldName].value = value;}; 함수 본문 1 해설함수 handleChange는 fieldName를 받아 새로운 함수를 반환한다. 반환한 함수에 value를 전달하는 경우, fields 객체에서 fieldName에 해당하는 프로퍼티에 value를 전달받은 value로 갱신한다. 사용 예시 2123456const createForm = createElHtml('form');const createInput = createElHtml('input');const form = createForm();const idInput = createInput('name=&quot;username&quot;');form.innerHTML = idInput; 함수 본문 21234const createElHtml = (tag) =&gt; ( arrtibutes = '', children = '',) =&gt; `&lt;${tag} ${arrtibutes}&gt;${children}&lt;/${tag}&gt;`; 사용 예시 31234567891011121314151617const toBoolean = (x) =&gt; !!x;const double = (n) =&gt; n * 2;const toNumber = (str) =&gt; Number.parseInt(str, 10);const composedFunction = compose( toBoolean, double, toNumber,); // toNumber -&gt; double -&gt; toBoolean 순으로 실행된다.// 매개변수는 composedFunction에 전달된 것부터// 각 함수의 반환값이 다음 함수의 매개변수가 된다.composedFunction('0'); // falsecomposedFunction(''); // falsecomposedFunction(); // falsecomposedFunction('hello'); // falsecomposedFunction('2'); // true 함수 본문 31234const compose = (...functions) =&gt; [...functions].reduce((f, g) =&gt; (...args) =&gt; f(g(...args)), ); 함수 본문 3 해설 compose(f,g,h)(1) =&gt; f(g(h(1)))의 형태로 실행한다. compose 내부에선 functions를 배열에 넣어 reduce를 호출한다. Array.reduce는 (result, currentvalue) =&gt; result; 이다. reduce에게 (f, g) =&gt; (...args) =&gt; f(g(...args))를 인자로 넘겨준다. 단계별 흐름도 (f, g) =&gt; (...args) =&gt; f(g(...args)) // f, g ((...args) =&gt; f(g(...args)), h) =&gt; ((...args) =&gt; f(g(h(...args)))) // f,g 합성함수, h 함수 함수형 프로그래밍과의 연관성 currying은 함수를 인자로 받게 만들 수도 있고, 함수가 아닌 값을 받게 만들 수도 있다. 두 방법 모두 유용하다. currying은 마지막으로 반환되는 경우를 제외하면 대부분 계속해서 함수를 반환한다. 이는 함수간의 조합이 쉽다. 참고 JavaScript ES6 curry functions with practical examples (EN) Learning Javascript Courses: ES6 Curry (EN) What is the advantage of currying? (EN)","link":"/fjs-2-currying/"},{"title":"함수형 자바스크립트 기본","text":"함수형 패러다임을 자바스크립트에 적용할 때에 알면 좋은 Javascript의 기본 문법과 배경 지식을 설명한 글이다. 자바스크립트와 타 언어들간의 함수형 프로그래밍 지원의 차이Javascript에는 없는 함수형 언어의 특징 순수 함수 강제 (side effect를 발생시키는 표현식을 허용하지 않음) 불변성 강제 (변수라고 부르지만, 상수. 객체까지도.) 재귀 강제 (반복문 미지원) 아무래도 Javascript는 멀티 패러다임이라 함수형 패러다임만 지원하는 언어에 비해서는 제약이 덜할 수 밖에 없을 것이다. Javascript에서 이를 극복하는 방법 eslint 규칙 사용으로 부분적으로 극복. ImmerJS 등의 라이브러리 도입 함수형 프로그래밍에 익숙해진다면, 괜찮을 것. (1과 마찬가지로 eslint 규칙 도입 등.) Javascript 런타임들이 대부분 꼬리 재귀 호출 최적화를 지원하지 않기 때문에, 재귀가 타 함수형 언어에 비해 성능이 낮을 수 밖에 없다. 꼬리 재귀 호출 최적화는 재귀 호출이 함수의 마지막에서 발생하는 경우에 적용된다. 컴파일러가 자동으로 재귀를 반복문으로 치환한다. 덕분에 스택 프레임을 1개만 사용한다. Javascript의 장점 다른 모든 함수형 언어는 학습 곡선이 높다. 누구나 이해하고 사용할 수 있다고 하기 힘들다. ES6+부터 함수형 지원이 좋은 편이다. 타 함수형 언어들에 비해 시장이 크고, Production-level Application 구축이 용이하다. 함수 실행(call, apply)과 인자(arguments), 점(.) 다시 보기Javascript의 함수 안에서는 arguments 객체와 this 키워드를 사용할 수 있다. ※ 화살표 함수에서는 arguments를 사용할 수 없다. 123Uncaught ReferenceError: arguments is not defined at hi (&lt;anonymous&gt;:1:37) at &lt;anonymous&gt;:1:1 arguments 객체배열과 유사한 Arguments 객체(Arguments(4) [ ‘a’, ‘b’, ‘c’, ‘d’, … ])로 매개변수들이 전달된다. arguments에 접근하는 시점에 따라 값이 변경될 수 있다. Javascript의 parameter는 변경할 수 있기 때문에, 이를 변경 후 arguments를 찍어보면 다르게 나온다. 12345function hello(a, b) { a = 1; console.log(arguments);}hello('a', 'b'); // Arguments [1, 'b']; this 객체obj.prop()으로 호출 시 obj가 this가 된다. . 좌측의 객체가 항상 this가 된다. 최상단 scope에서 호출하면 기본적으로 window., global.이 생략된 것이기 때문에 this가 window, global이 된다. (global은 Node.js 환경에서.) const { prop } = obj; prop(); 하면 prop이 obj에 속해있음에도 불구하고 this가 window, global이 된다. Function.prototype.callthis 객체를 지정해서 함수를 호출할 수 있다. 12345678const obj = { thisIs: 'obj',};function hello(a, b) { console.log(this);}// 매개변수는 가변 매개변수여서 개수 제한 없이 전달 가능하다.hello.call(obj, 'a', 'b'); // { thisIs: &quot;obj&quot; } Function.prototype.applycall과 다른 점은 매개변수를 배열과 유사한 객체로 넘겨야 한다는 점이다. 12345678910111213const obj = { thisIs: 'obj',};function hello(a, b) { console.log(this);}// 매개변수는 배열과 같은 객체로 전달하면 된다.hello.apply(obj, ['a', 'b']); // { thisIs: &quot;obj&quot; }hello.apply(obj, { 0: 'a', 1: 'b', length: 2,}); // { thisIs: &quot;obj&quot; }","link":"/fjs-3-functional-js-basics/"},{"title":"순수함수와 curry 함수","text":"이번 글은 쉽다(?). 순수함수에 대해 이론적으로 다루고, curry 함수를 소개한다. 다만 객체지향과의 비교, 테스트와 설계에 대한 내 생각을 공유하므로 지식이 없으면 어렵게 보일 수도 있다. 순수함수?순수함수는 수학에서 정의하는 함수와 동일하다. 입력에 대한 출력이 항상 동일하고, 입력에 대한 출력이 항상 1가지이다. 이게 가능하기 위해선 DB, HTTP, 현재 시간 등에 의존하면 안 된다! 함수 외부의 것과 함수 내용이 전혀 연관이 없어야 한다. 부원인과 부작용영어권에서 흔히 side-effects라고 얘기하는 부수효과나 부작용은 함수 밖의 코드의 상태에 영향을 주는 일을 말한다. 부수효과를 크게 부작용과 부원인으로 구분할 수 있다. 부작용은 숨겨진 출력이고, 부원인은 숨겨진 입력이라고 생각하면 된다. 왜 외부의 상태와 상호작용하면 안 될까? 궁금하면 계속 읽어야 된다. 숨겨진 입력숨겨진 입력이라고 하면 뭐가 있을까? Javascript와 같이 객체지향 패러다임을 지원하는 언어의 경우는 this가 항상 함수에 전달된다. this도 숨겨진 입력이다. 또한 함수 내부에서 new Date() 등의 코드로 현재 시간에 의존하는 경우도 숨겨진 입력이라고 할 수 있다. 둘 모두 외부의 상태를 변경하기 때문이다. 숨겨진 출력숨겨진 출력은 함수를 실행했을 때 바뀌는 모든 것이라고 할 수 있다. 순수함수 내에서는 어떤 외부의 상태도 변할 수 없으므로, 어떤 외부의 상태가 조금이라도 변경된다면 그 함수는 순수하다고 할 수 없다. 부수효과는 복잡성 빙산왜 외부의 상태와 상호작용하면 안 될까? 순수함수가 아닌 함수의 Signature는 프로그래머가 읽더라도, 심지어 객체지향 언어의 설계 방식대로 설계했더라도 무슨 부수효과가 일어날지 알 수 없다. 캡슐화는 좋은 규칙이지만 그 구현 코드를 읽기 전까지 부수 효과를 정확히 알 순 없다. 부수효과가 왜 복잡성 빙산일까? 프로그래머가 예상한 그대로 동작하지 않는 경우 논리적 버그의 원인이 되기 때문이다. 부수 효과는 해당 코드 혹은 해당 코드와 간접적으로 연관이 있는 코드를 수정했을 때 바뀌기 또한 쉽고, 바뀌었을 때 작동하지 않게 될 확률도 높다. 그래서 객체지향 방식으로 설계를 하는 경우 회귀테스트를 그렇게 많이 작성해야 하나 보다. 응집성과 캡슐화를 생각해서 상태 의존적이고, 변경 시 서로의 영향을 받아서 깨지기 쉽기 때문이다. 그래서 함수형 패러다임에서는 공유 자체를 하지 않는 방향으로 설계하도록 지향한다. 그 결과가 순수함수이다. 순수함수가 아니면 테스트하기 힘들다어떤 함수가 부수효과가 있는 경우 이미 그 함수는 다른 코드랑 최소한 1번은 엮여 있을 수 밖에 없다. 덕분에 그 함수를 테스트하기 위해서는 다른 코드까지 테스트할 수 밖에 없고, 이 과정에서 Blackbox Testing이 불가능해진다. 구현 상세에 외부 코드와의 연관이 존재하기 때문이다. 이 과정은 객체지향 언어로 작성한 경우 자주 발생하며 덕분에 Mock을 자주 사용하게 된다. 또한 테스트 자체도 구현 상세의 변경에 취약하게 된다. 부수효과를 제거하기, 제거했을 때의 장점부수효과를 제거하려면 순수함수를 만들고 사용하면 된다. 모든 부작용, 부원인은 숨겨진것이기에 이를 Signature에 명시하면 된다. 이렇게 명시하는 것은 객체지향 언어에서는 응집성과 캡슐화를 위해 구현 상세로 분류하여 함수 안에 전부 집어넣는 등 지양하는 편이지만, 함수형 패러다임에서는 권장된다. 덕분에 덜 복잡해지고, 훨씬 테스트하기 쉬워지며, 추론이 훨씬 쉬워지기 때문이다. 부수효과를 완전히 제거할 수는 없다.아무래도 웹 등 실세계의 애플리케이션은 함수 내의 수식을 한 번 계산하고 종료하는 게 목적이 아니라, 부수효과로 불리는 것들 대부분을 사용하여 목적을 달성할 수 밖에 없다. 함수형 패러다임은 이런 한계를 인정하고, 가능한 모든 곳에서 부수효과를 제거하고, 제거할 수 없을 땐 강력히 통제한다. 순수함수의 조합과 재사용성순수함수는 그 자체의 명료함 덕분에 재사용성과 조합이 굉장히 쉽고, 많이 조합하더라도 쉽게 그 결과가 예측 가능하다. 특히 한 번에 풀 수 없는 크고 복잡한 문제를 쪼개서 작은 함수의 조합으로 해결할 수 있다. 앞서 만들어 놓은 산출물을 쉽게 조합하여 새로운 문제를 해결할 수 있게 되고, 생산성도 비약적으로 늘어난다. 순수함수에 대한 간단한 사실들 순수함수는 수학의 함수와 동일한 정의를 갖는다. 순수함수는 (input, output) 쌍이므로 객체로도 표현 가능하다. (key,value 쌍) 순수함수는 항상 캐시 가능하다. 순수함수는 필요한 건 다 전달받는다(dependency injection) 동시성 문제가 적거나 없다. 공유하는 메모리가 없기 때문이다. curry 함수2번째 글에서 currying을 이미 다루었다. 그 때의 currying은 프로그래머가 함수에 대해 직접 curry한 방식이고, 이번에는 어떤 함수에 대해 알아서 curry된 함수를 반환하는 함수를 소개한다. curry 함수는 함수를 받아, 인자가 완전히 전달되지 않은 경우 남은 인자를 받을 함수를 반환한다. curry 함수의 구현은 function.length와 bind, apply를 사용하는 게 핵심이다. 1234567891011function curry(f) { const len = f.length; return function $curry() { if (arguments.length &lt; len) { // 원래 함수의 매개변수의 갯수보다 $curry에 전달된 매개변수의 갯수가 작은 경우. return $curry.bind(null, ...arguments); // $curry에 계속 전달받은 매개변수들이 bind 된다. (arguments가 계속 쌓인다.) } else { return f.apply(null, arguments); // 실제 함수 호출. } };} 12345678910// 예시const add = (a, b, c) =&gt; a + b + c;const addC = curry(add);const add1 = addC(1);const add1and2 = add1(2);const add1and2and3 = add1and2(3);console.log(addC(1, 2)); // function $curryconsole.log(add1(2)); // function $curryconsole.log(add1and2and3); // 6console.log(add1and2(3) === add1and2and3); // true 참고ES6 bind 함수 (KO)","link":"/fjs-4-pure-functions-and-curry/"},{"title":"함수형 패러다임의 꽃: 함수 합성(composition)","text":"함수형 패러다임에서 최우선 설계 원칙으로 삼아진다고 하는 함수들의 합성에 대해서 설명한다. 합성이전에 설명했던 compose 함수를 말한다. 1234const compose = (...fns) =&gt; fns.reduce((f, g) =&gt; (...args) =&gt; f(g(...args)), ); 합성은 왜 하는걸까?프로그램을 간결하고 실용적으로 작성할 수 있게 한다. 합성이 되므로 함수를 부담 없이 나눌 수 있게 되어 더 작고 의미있는 단위의 함수를 더 편하게 작성할 수 있다. 이렇게 합성된 함수는 가독성이 좋다. 아무래도 객체지향 패러다임을 강하게 지원하는 언어들에선 함수 합성이 쉽지 않다. 애초에 순수 함수를 작성하기도 쉽지 않다. public static으로 도배할 순 없기 때문이다. 합성함수의 결합법칙함수 합성은 수학에서의 합성함수와 같이 결합법칙이 성립한다. compose(f, compose(g, h)) === compose(compose(f, g), h)가 성립한다. Javascript 상에서 생성되는 함수가 동일하다는 것이 아니라, 그 실행 결과가 언제나 같다는 뜻이다. 결합법칙이 무슨 소용일까합성한 함수들을 재귀적으로 합성한 경우, 결합법칙을 적용하면 결과 예측과 리팩토링 시에 유용하다. 그 예로, 아래 세가지 loudLastUpper 함수는 동일하다. 더 작고 더 의미있는 함수로 정의할수록 재사용성과 가독성은 높아진다. 버전 1 123456const loudLastUpper = compose( exclaim, toUpperCase, head, reverse,); 버전 2 (리팩토링) 123456const last = compose(head, reverse);const loudLastUpper = compose( exclaim, toUpperCase, last,); 버전 3 (리팩토링) 123const last = compose(head, reverse);const angry = compose(exclaim, toUpperCase);const loudLastUpper = compose(angry, last); 쓸모있고 재미있는 디버깅 방법합성 함수를 디버깅하는 재밌는 방법이 있다. 흔히 trace라 부르는 유명한 함수인데, 항등함수(const pass = x =&gt; x;)에 console.log만 추가한 함수이다. 123456789101112131415161718192021const trace = (tag) =&gt; (x) =&gt; { console.log(tag, x); return x;};// 자동 커리const trace = curry((tag, x) =&gt; { console.log(tag, x); return x;});// 사용 예시const toDebug = compose( replace, trace('after A'), applyA, trace('after B'), applyB, trace('after last'), last,); 당연하게도 trace 함수는 순수하지 않다. console를 사용하기 때문이다. 간단한 함수 합성 예제이하의 예제 코드는 아래의 cars 객체를 대상으로 한다. 123456789[ { name: 'Aston Martin One-77', horsepower: 750, dollar_value: 1850000, in_stock: true, }, //...]; 예제 1각 함수들의 정의는 이 문서를 참고하라. 이 문서는 ramdajs documentation과도 호환된다. 12345678910const isLastInStock = (cars) =&gt; { const lastCar = last(cars); return prop('in_stock', lastCar);};// after compose:const isLastInStock = compose( prop('in_stock'), last,); 예제 212345678910111213141516const average = (xs) =&gt; reduce(add, 0, xs) / xs.length;const averageDollarValue = (cars) =&gt; { const dollarValues = map( (c) =&gt; c.dollar_value, cars, ); return average(dollarValues);};// after compose:const averageDollarValue = compose( average, map(prop('dollar_value')),); 예제 312345678910111213const fastestCar = (cars) =&gt; { const sorted = sortBy((car) =&gt; car.horsepower); const fastest = last(sorted); return concat(fastest.name, ' is the fastest');};// after compose:const fastestCar = compose( append(' is the fastest'), prop('name'), last, sortBy(prop('horsepower')),); 함수 합성 예제 프로그램스펙 검색어에 대응하는 URL을 생성한다. flicker API를 호출한다. 결과 JSON에서 이미지 링크를 추출한다. 이미지를 HTML에 표시한다. 구현 코드예제의 스펙에서 보았듯, 2단계 API 호출과 4단계 이미지 표시는 순수하지 않다. 일단 순수하지 않은 함수를 같이 사용하면서 예제를 구현한다. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758// 유틸 함수 선언const prop = curry((p, obj) =&gt; obj[p]);// 순수하지 않은 함수// Impure 객체로 접근하도록 하여 사용자에게 주의를 준다.const Impure = { getJSON: curry((callback, url) =&gt; $.getJSON(url, callback), ), setHtml: curry((sel, html) =&gt; $(sel).html(html), ),};/* 참고: 서버의 응답이 아래와 같은 형태로 구성됨 { ... items: [ { ... media: { m: '&lt;image-link&gt;' } }, { ... }, { ... }, ] }*/const host = 'api.flicker.com';const path = '/services/feeds/photos-public.gne';const query = (t) =&gt; `?tags=${t}&amp;format=json&amp;jsoncallback=?`;const url = (t) =&gt; `https://${host}${path}${query(t)}`;const mediaUrl = compose( prop('m'), prop('media'),);const mediaUrls = compose( map(mediaUrl), prop('items'),);const img = (src) =&gt; `&lt;img src=&quot;${src}&quot; /&gt;`;const render = compose( Impure.setHtml('#root'), map(img), mediaUrls,);const app = compose(Impure.getJSON(render), url);app('cat'); compose와 map 리팩토링아주 간단한 리팩토링이다. 같은 배열에 대해 map을 여러 번 실행하기보다, 순서를 유지한 채로 매 원소에 대해 map할 함수를 합성해서 한 번에 실행하게 되면 반복 횟수를 줄일 수 있다. 1234// fromcompose(map(img), map(mediaUrl));// tocompose(map(compose(img, mediaUrl))); 참고mostly-adequate-guide (EN)","link":"/fjs-5-composition/"},{"title":"JS Async Functionality 1 - Intro","text":"이 글은 자바스크립트에서 비동기를 다룰 때 마주치는 개념들인 Promise, Generator, Async-Await을 큰 범위에서 다룬다. 중간 중간에 재밌는 패턴들도 수록했다. Why Promise? What’s Promise?Promise는 순차적인 비동기 코드를 깔끔하게 짤 수 있게 하는 문법이다. 문법에 포함된 Promise 객체로 처리한다. Promise로 거의 모든 비동기를 처리한다고 해도 과언이 아니다. Promise가 익숙하지 않다면 MDN을 참고:https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Promise &gt; https://developer.mozilla.org/en-US/docs/Learn/JavaScript/Asynchronous/Promises 비동기 작업 시 순차적인 흐름을 많이 구현해야 하는데, CPS 방식으론 간결하게 짤 수 없다. CPS 패턴 사용 시의 Tip | JSQnA 참고. Promise의 장점: (콜백과 관련한 비교에 대한 내용은 CPS 패턴 참고.) 프로미스 체인을 사용하면 작업들을 순차 실행시키는 일은 그리 어렵지 않다. throw를 프로미스 체인에서 사용할 수 있다. catch 될 때까지 전체 체인에 오류를 자동으로 전파할 수 있다. 비동기 오류가 누락될 확률이 줄어든다. 동기적으로 값을 반환해도 비동기적인 호출을 보장한다. 함수가 동기, 비동기 반환을 섞어서 하는 것은 나쁘다. Promise.all 함수를 통해 비동기 작업을 병렬로 실행할 수 있다. (이건 CPS도 가능) Promise.race 함수를 통해 비동기 작업 중 가장 먼저 수행이 끝난 결과만 사용할 수 있다. (CPS에선 직접 구현해야 함.) Promise로 함수 배열을 순차적으로 실행하는 패턴 (현재 이해 부족으로 인해 수정 필요함.): 책에 재밌는 코드가 있어 가져왔다. Promise로 함수의 배열을 순차적으로 실행하는 방법이 있을까? 12345678910111213// sequential :: Array(() =&gt; Promise) =&gt; Promisefunction sequential(tasks) { // 빈 값을 반환하는 Promise를 생성한다. let promise = Promise.resolve(); tasks.forEach((task) =&gt; { // promise에 then으로 체인을 걸고, // 다음 순번의 '이전 작업'이 되기 위해 promise 변수로 할당한다. // UPDATE: task는 Promise를 반환하는 함수여야 한다. promise = promise.then(task); }); // 최종 Promise를 반환한다. return promise;} reduce로도 가능하다: 1234567891011const tasks = [ /* ... */];let promise = tasks.reduce( (prev, task) =&gt; prev.then(task), Promise.resolve(),);promise.then((result) =&gt; { // TODO: retreive result}); 제한된 개수로 병렬 실행하기: (현재 이해 부족으로 인해, 추후 삽입 예정) ES8 비동기 함수정의에 대한 자세한 내용은 MDN async function, MDN AsyncFunction 생성자를 참고하라. ES7 비동기 함수는 비동기적으로 동작하는, async, await 문법이 활용된 함수이다. (설명 보충 예정.) Why Generator? What’s Generator? Generator는 시작 지점이 여러 개이며 중간에 실행을 정지/재개할 수 있는 함수이다. 시작 지점이 여러 개: 다른 시작 지점에 대해 매번 새로운 arguments로 호출할 수 있다. 정지/재개할 수 있다: 제너레이터 함수는 실행 후 값을 반환할 때 정지한다. 이후 호출하면 다시 재개된다. Generator가 익숙하지 않다면 MDN을 참고: https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Statements/function* https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Generator 시작하기 전에 아래 두 코드의 결과를 모르겠다면 이후 내용을 이해하기 어려우므로, Generator에 대해 추가적으로 공부를 하기 바란다. 1234567891011121314151617181920212223242526272829// Generator Example 1function* fruitGenerator() { yield 'apple'; yield 'orange'; return 'watermelon';}const newFruitGenerator = fruitGenerator();console.log(newFruitGenerator.next());console.log(newFruitGenerator.next());console.log(newFruitGenerator.next());// Generator Example 2function* iteratorGenerator(arr) { for (let i = 0; i &lt; arr.length; i++) { yield arr[i]; }}const iterator = iteratorGenerator([ 'apple', 'orange', 'watermelon',]);let currentItem = iterator.next();while (!currentItem.done) { console.log(currentItem.value); currentItem = iterator.next();} Generator with CPS into Async-Await:놀랍게도 Generator에 약간의 양념을 치면 ES7 비동기 함수를 만들어낼 수 있다. 12345678910111213141516171819202122232425262728293031323334const fs = require('fs');const path = require('path');// 제너레이터로 비동기 흐름을 구현하는 방법이다.function asyncFlow(generatorFunction) { // callback 함수는 비동기 함수에 CPS 패턴으로 넘겨져서, 결괏값으로 다시 제너레이터를 호출하는 데 사용된다. function callback(err) { if (err) { return generator.throw(err); } const results = [].slice.call(arguments, 1); generator.next( results.length &gt; 1 ? results : results[0], ); } const generator = generatorFunction(callback); generator.next();}// asyncFlow, callback을 감추고, yield를 await으로 바꾼다면 async-await과 같은 문법을 지닌다.asyncFlow(function* (callback) { const fileName = path.basename(__filename); const myself = yield fs.readFile( fileName, 'utf8', callback, ); yield fs.writeFile( `clone_of_${fileName}`, myself, callback, ); console.log('Clone created');}); try-catchAsync-Await과 유사하게, 제너레이터에는 throw API가 있는데, 제너레이터 함수 내에서 try-catch로 이를 처리할 수 있다: 123const twoWay = twoWayGenerator();twoWay.next(args); // args를 전달twoWay.throw(new Error()); // throw로 Error 객체 전달. 제너레이터 함수 내의 catch 절로 이동하게 된다. 참고자료 전문: Async-Await ≈ Generators + Promises 참고 2: Difference between async/await and ES6 yield with generators | StackOverFlow 참고 3: ES2017 - Async vs. Yield | StackOverFlow 하나의 API로 CPS와 Promise 모두 지원하는 방법mongoose와 같은 많은 라이브러리는 CPS와 Promise 방식을 모두 지원한다. 어떻게 한 함수로 동시에 지원할 수 있을까? 아래 코드와 같이 구현한다면 가능하다. 1234567891011121314151617181920212223242526272829// 마지막 인자로 callback 함수를 받는다.// Promise로 사용하는 경우 callback 함수를 넘기지 않으니, 상관 없다.function asyncDivision(dividend, divisor, cb) { // 항상 Promise를 반환한다. // 어차피 CPS 패턴을 사용하는 코드라면 Promise로 결과를 받아서 처리하지 않는다. return new Promise((resolve, reject) =&gt; { // 비동기로 반환 process.nextTick(() =&gt; { const result = dividend / divisor; if ( isNaN(result) || !Number.isFinite(result) ) { const error = new Error( 'Invalid operands', ); if (cb) { cb(error); } // 콜백이 있으면, 콜백을 호출한다. return reject(error); // 콜백이 있든 없든, Promise reject로 catch 체인을 실행한다. } if (cb) { cb(null, result); } // 콜백이 있으면, 콜백을 결과로 호출한다. resolve(result); // Promise resolve로 then 체인을 실행한다. }); });} 장점: Promise, CPS 패턴 사용자 모두에게 기능을 제공할 수 있다. 비동기와 함수형 자바스크립트Javascript는 순수한 함수형 언어가 아니므로, 모든 코드를 함수형 패러다임을 적용해서 작성할 수 없다고 한다. 비동기를 다루는 코드에 있어서는, 특히 async-await 키워드를 사용하여 작성할 때는 명령형 코드가 되므로, 더 함수형과 멀어지게 되는데, 결론적으론 Javascript에서 함수형 패러다임을 실천할 때에는 함수형인 코드 베이스와 그렇지 않은 부분으로 나누는 게 좋다고 한다. 또한 Promise든 Async-Await이든 하나를 택해서 통일하는 게 좋다고 하니 참고 바란다. 전문: JS: Promises, async/await, and functional programming. TODO: Generator는 아직도 공부 중이다. Iterable 프로토콜에 대한 얘기도 있고, 비동기 처리 외에 Generator의 쓰임새나 Generator 자체 개념에 대해 더 공부해야 한다. 코루틴에 대해서도 공부해봐야 할 것 같다. 공부 중 접하게 된 키워드이다. 제너레이터에 대한 설명을 보강해야겠다. 이해가 완료되면 자체 제작한 예제 코드로 교체한다.","link":"/js-async-1/"},{"title":"JS Async Functionality 2 - Promise","text":"이 시리즈는 자바스크립트에서 비동기를 다룰 때 마주치는 개념들을 다룬다. 이번 글에서는 Promise를 다룬다. JS Async Functionality 1 - Intro Promise:{ pending, fulfilled, rejected }상태를 가지는 객체로, executor 함수를 인자로 받는다. executor 함수( (resolve, reject ) =&gt; {} )의 역할: 비동기 함수를 호출하고 그 비동기 함수의 콜백에서 resolve를 호출한다. 12345678910111213const fetch = url =&gt; new Promise((resolve, reject) =&gt; { // 이 함수가 executor 함수이다. (주석 설명 참조) // 1. example Async API provided by Node.js http.get(options, result =&gt; { let data; result.on('data', chunk =&gt; data += chunk); // 2. Calls either [ resolve, reject ] from async callback. result.on('end', () =&gt; resolve(data)); result.on('error', err =&gt; reject(err)); });}); executor 함수는 기존의 비동기 처리 방식을 그대로 옮겨온 것으로 이해하기 어렵지 않다. 다만 Promise Chaining이라는 개념으로 Callback Hell을 1차원으로 들여쓰기 단계를 낮출 수 있다. 이렇게 들여쓰기 단계를 줄이는 것은 중요한데 가독성에 의한 논리 오류가 빈번하게 발생했기 때문이다. 또한 Javascript 특성상 CPS 패턴으로 작성된 비동기 처리 함수에서, 이후에 호출되는 함수는 이전 함수의 클로저 참조도 할 수 있다. 부주의하게 클로저 영역의 변수들을 사용하는 경우 메모리 사용량 면에서 좋을 게 없었다. (ex) 전체 비동기 절차가 끝날 때 까지 호출 함수의 지역 변수들이 해제되지 못하는 등. Promise Chaining:Promise는 타입이자 객체이다. Promise(); 인스턴스는 자신의 실행 흐름에 관여하는 메소드를 세 개 갖는다: then, catch, finally (참고로 메소드는 총 5개이다. race, all이 그 나머지 둘이다.) Prototype에 등록된 함수로, then은 Promise를 반환하고(그래서 Chaining이 가능하고) catch는 reject된 Promise에 한 해 수행되는 조건문으로 then(undefined, onRejected)와 동등하다. finally는 JS의 try-catch-finally의 finally와 동등하다. thenthen = (onFulfilled, onRejected) =&gt; Promise 즉 then의 두 번째 인자는 catch 절인 셈이다. 웬만하면 가독성을 위해 따로 catch 절을 사용한다. onFulfilled = value =&gt; Promise (여기서 value는 Promise가 resolve한 값이다. 보통 비동기 함수의 결괏값.) onRejected = value =&gt; {} (여기서 value는 Promise가 reject한 값이다. 보통 Error 객체.) 1234567p.then(onFulfilled, onRejected);p.then(function(value) { // 비동기가 별 탈 없이 진행된 경우.}, function(reason) { // 비동기 함수 수행 중 오류가 난 경우.}); then에서는 값을 그냥 반환하는 경우 Promise.resolve로 감싼 것과 같다. 즉 Promise가 반환되는 것인데, 그렇기 때문에 Promise Chaining이 가능한 것이다. catchcatch = onRejected =&gt; Promise (!) catch 메소드는 try-catch의 catch와 같은 역할이다. 즉, catch가 성공했느냐, 실패했느냐에 따라 다시 then이 실행될 수도 있고 다른 catch가 실행될 수도 있고 앱이 멈출 수도 있다. Case 1: catch 절에서 resolved Promise를 반환하는 경우: 이후의 then 수행 catch 절에서 따로 throw를 하거나, Promise.reject()를 호출하지 않는 경우 Promise는 resolved 상태로 변하여 then을 수행한 것과 동등하게 된다. Case 2: catch 절에서 rejected Promise를 반환하는 경우: 이후의 catch 수행 JS의 try-catch에서 catch는 여러 개가 존재할 수 없는 것에 비해 Promise가 rejected 상태이면 catch절은 계속해서 호출된다. 보통 여러 개의 catch 절은 특정 오류만 잡고 싶을 때 사용한다. 1234567891011121314151617var p1 = new Promise(function(resolve, reject) { resolve('Success');});// 아래처럼 pin-point로 catch 절을 사용하는 것이 가능하다.// 기존 JS에서 try-catch를 여러 번 순차적으로 사용한 것과 동등하다.// 1.Promise를 생성p1.then(function(value) { // 1. throw를 호출해 catch 절로 이동 throw new Error('oh, no!');}).catch(function(e) { // 2. reject 혹은 throw를 하지 않으므로 then 수행 console.error(e.message); }).then(function(){ // 3. 이 then이 수행되게 됨. console.log('after a catch the chain is restored');}).catch(function () { // 4. 만약 [2], [3]에서 throw를 하는 경우 여기로 오게 됨. console.log('Not fired due to the catch');}); finally(설명 생략) 알기 어려운 Promise의 특징1. then, catch는 비동기로 실행된다. 아무리 Promise.resolve(); 로 resolve가 동기로 수행되더라도 then, catch는 비동기로 queue 된다. 1234567Promise.resolve(null).then(v =&gt; console.log('Async: ' + v));console.log('Sync!');// 결과:// Sync!// Async: null// WHY? then이 async로 microtask queue에 들어갔기 때문.// then이 2: then, catch는 비동기이지만 한꺼번에 수행된다. 만약 then, catch가 setTimeout과 같은 일반적인 비동기였다면 Task Queue에서 처리된다. Task Queue는 한 작업만 처리하고 나머지 작업은 다음 순서로 넘긴다. 123456function loop() { setTimeout(loop, 0);}loop();// 무한 루프에 걸리지 않는다.// Microtask가 아니므로 이벤트 루프에서 한 작업씩만(!!!) 처리한다. 그러나 Promise, then, catch는 Microtask Queue에서 수행되는데, 이는 Event Loop 내의 Event Loop으로 생각하면 된다. 굳이 이렇게 하는 이유는 다른 Javascript 수행이 되지 않음을 보장 화면이 변경되지 않음을 보장 하기 위해서이다. Microtask가 호출한 microtask 역시 이어서 수행되며 microtask queue가 빌 때까지 이 단계는 끝나지 않는다. 1234567function loop() { // then은 microtask에 queue 된다. Promise.resolve().then(loop);}loop();// microtask는 현재 cycle에서 microtask가 비워질 때까지 수행을 멈추지 않는다.// 즉 무한 루프를 비동기 코드로 발생시킬 수 있는 셈이다. 출처: 이벤트 루프와 매크로, 마이크로 태스크, Jake Archibald: Inside Loop - JSConf.Asia | Youtube Promise가 연속적으로 수행되어 문제가 발생하는 예제를 생각하려고 했으나 대부분의 비동기는 microtask를 사용하지 않기에 큰 문제는 없을 것 같다. 따라서 이 본문의 내용을 몰라도 거의 문제는 없을 것 같다. 출처Promise then Promise catch (재밌는 점은 catch 문서는 한국어 번역이 없다는 점이다.) Using Promises TODO처음에 React를 통해 ES6를 배우면서 Promise를 접했을 때보다 문서 개수나 번역이 훨씬 좋아졌다는 걸 느꼈다. 앞으로의 JS 표준을 다루는 MDN Wiki 문서가 있으면 나도 기여해야겠다","link":"/js-async-2/"},{"title":"이벤트 루프는 환경마다 다를까?","text":"Javascript의 모든 코드는 이벤트 루프에서 처리된다. 그리고 Node.js의 이벤트 루프 구현체로 libuv가 사용 된다는 사실은 널리 알려져 있다. 그런데 이상하지 않은가? 왜 Node.js를 설명할 때 이벤트 루프의 구현체라며 따로 소개하는 걸까? 과연 브라우저 상의 이벤트 루프와 Node.js의 이벤트 루프는 동일하다고 생각해도 될까? 이벤트 루프 구현체가 다르다는 것을 어떻게 생각하면 좋을까? 원인은 환경 차이이벤트 루프의 처리 방식은 스펙으로 결정돼있지만 벤더마다 약간씩 다르게 구현하는 부분이 있다고 한다. Node.js 또한 예외는 아니다. 기능 브라우저 NodeJs File, Network I/O No Yes Event의 종류 Mouse, Keyboard Events File, Network I/O 이벤트 루프 구현체 libevent (크롬 기준) libuv 자바스크립트 엔진 V8 (크롬 기준) V8 process.nextTick (process 모듈 자체 API) No Yes setImmediate (ECMA 표준 아님) No Yes 위 표 이외에 처리 순서의 차이가 약간 있긴 하지만 Node v11부터는 그 차이마저 적은 편이다. 결론: 환경의 차이 때문에 약간의 구현 상의 차이가 있다 정도로 받아들이자. 출처: JavaScript Event Loop vs Node JS Event Loop","link":"/js-event-loop-browser-vs-node/"},{"title":"Javascript의 Generator","text":"이 글은 자바스크립트의 제너레이터 문법에 대해 간략히 소개한다. 정의JavaScript의 제너레이터는 function* 으로 정의된 제너레이터 함수가 반환한 객체이다. 이 객체는 이터레이터(iterator)이다. 123456789function* foo() { yield 1; yield 2; yield 3;}// foo()로 생성된 제너레이터를 순회하며 값을 읽어간다.for (let i of foo()) { console.log(i);} Generator 객체와 함수(팩토리)제너레이터 함수를 호출하면 제너레이터 객체를 반환하고 끝난다. 제너레이터 객체의 next(...args)를 통해 제너레이터의 본문을 일정 부분 실행할 수 있다. 이터레이터는 next() 함수로 파라미터를 전달할 수 없다. 제너레이터가 값을 읽을 수 있기 때문에 협력적 멀티 태스킹이 가능하다. Generator 기반 협력적 멀티 태스킹협력적 멀티 태스킹은 코루틴에 나오는 개념이다. (추후 정리할 예정이다.) 1234567891011121314go(function* producer() { for (let i = 0; i &lt; 10; i++) { yield write(i); yield sleep(100); // -- sleep이 가능해진다! }});go(function* consumer() { let v; while ( typeof (v = yield read()) !== 'undefined' ) { console.log('read:', v); }}); 이 코드의 go와 같은 함수를 제너레이터 실행기라고 한다. co 라이브러리가 훌륭한 제너레이터 실행기를 제공한다. 제너레이터 실행기는 원래 동기적으로 수행되는 제너레이터를 비동기 호출을 수행하게 만든 다음 callback을 통해 다시 제너레이터를 호출하게끔 하여 비동기 코드를 동기 코드처럼 작성할 수 있게 하는 목적의 함수이다. 제너레이터 실행기12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849// 제너레이터 실행기function grun(g) { const it = g(); (function iterate(val) { const x = it.next(val); if (!x.done) { if (x.value instanceof Promise) { x.value .then(iterate) .catch((err) =&gt; it.throw(err)); } else { setTimeout(iterate, 0, x.value); } } })();}// 제너레이터 실행기를 사용한 모습.// 꽤 async-await과 같이 가독성이 있다.function* theFutureIsNow() { let data; try { data = yield Promise.all([ nfcall(fs.readFile, 'a.txt'), nfcall(fs.readFile, 'b.txt'), nfcall(fs.readFile, 'c.txt'), ]); } catch (err) { console.error( 'Unable to read one or more input files: ' + err.message, ); throw err; } yield ptimeout(60 * 1000); try { yield nfcall( fs.writeFile, 'd.txt', data[0] + data[1] + data[2], ); } catch (err) { console.error( 'Unable to write output file: ' + err.message, ); throw err; }} Generator 직접 만들어보기자료 중 재밌는 것이 있었다: Babel은 Generator를 어떻게 바꾸나. 지금 나한테는 바로 이해하긴 어렵다. 직접 만들어보면 이해에 큰 도움이 될 듯 하다. Generator의 단점: Iterable은 가변 인자가 아니다제너레이터 객체는 위에서 말했듯 Iterable이지만, 이는 가변 인자와는 달라서, Math.min같은 함수를 이용할 때 spead 연산자로 배열로 만들어 넘겨야 하므로, 인자 전달 부분에서 아쉽다고 한다. 1234567891011const foo = function* () { /* ... */};// 제너레이터 객체만 넘길 순 없음.// 굳이 제너레이터 객체를 넘길거면, 받는 함수 입장에서 이터레이터를 써야 할 듯?// for-of 문으로.let min = Math.min(foo);// spread 연산자로 넘겨줘야 함.let min = Math.min(...foo()); Generator의 콜 스택? Calling .next() method just pushes that call on the top of the stack. It will then run the code inside the generator function. difference: it has to remember the state of all local variables, but engines already know how to do that from the implementation of closures. A generator call will restore the state so that it can continue where it left off. 일반적인 콜 스택과 동일하다고 한다. 다만 제너레이터 내에서 제너레이터를 호출하는 경우 복잡하다고 하는데, 거기까지 알 필요는 없을 듯 하다. 출처: Javascript stack model for generators | StackOverFlow 출처: Javascript Generator의 재미 (2016.12) 출처: Learning Javascript, O Reilly TODO:부족한 내용 보충하기. 제너레이터가 개념 뿐 아니라 사용이 중요한 개념이어서 정리가 난잡한데 다음주 중으로 정리하려고 한다.","link":"/js-generator/"},{"title":"Javascript의 라이센스","text":"이 글은 자바스크립트의 라이센스 - 소유권과 결정권에 대한 내용과 근거를 찾아 정리한 글이다. 1. Javascript의 라이센스Javascript 이해관계자는 크게 언어 명세을 결정하는 쪽과 언어를 개발하는 쪽으로 나뉜다. 주제 소유자 라이센스 생명 주기 언어 명세(ECMA 262) TC39 수정 하지 않는 선에서 사용 가능 매년 런타임(SW) 각 벤더 개별적으로 다름 상시 업데이트 재밌는 점은 TC39에 벤더들이 참여한다는 점이다. Trademark(유사 상표권)는 Oracle에 있는데, 명칭하는 데 사용하는 것은 문제가 되지 않는다고 한다. 언어 명세(구현 코드가 아님)은 ECMA International(TC39의 상위 개념)에서 제정한다. 언어 표준의 라이센스는 이 글 | StackExchange을 참고 언어 - 인터프리터, 컴파일러, 런타임 - 구현의 몫은 각 벤더사에 있으며 벤더사가 적용하는 라이센스에 따라 각자 라이센스를 가질 수 있다. 아래는 역사를 요약 설명한 내용이다. in 1995, Netscape decided to add a scripting language to Navigator. They pursued two routes to achieve this: collaborating with Sun Microsystems to embed the Java programming language, while also hiring Brendan Eich to embed the Scheme language. Netscape management soon decided that the best option was for Eich to devise a new language, with syntax similar to Java and less like Scheme or other extant scripting languages. Although the new language and its interpreter implementation were officially called LiveScript when first shipped as part of a Navigator release in September 1995, the name was changed to JavaScript three months later. 1995: Netscape Navigator라는 브라우저의 기능으로 스크립트 언어를 내장하기로 개발 In November 1996, Netscape submitted JavaScript to ECMA International, as the starting point for a standard specification that all browser vendors could conform to. This led to the official release of the first ECMAScript language specification in June 1997. 1996: Netscape -&gt; ECMA International로 소유권 이양 (이후 특정 기업의 소유는 아니게 됨) 출처: Javascript | Wikipedia 2. TC39 소개 Ecma International’s TC39 is a group of JavaScript developers, implementers, academics, and more, collaborating with the community to maintain and evolve the definition of JavaScript. TC39에서는 미팅 회의록을 공개하는데, 최근 미팅을 보니 주요 브라우저 벤더사만 참여하는 것은 아니고 회의마다 여러 주체나 외부 인사도 참여하는 듯 하다. TC39에서 Javascript의 스펙을 결정한다고 한다. This Standard defines the ECMAScript 2021 general-purpose programming language. TC39는 가장 최신의 Javascript 스펙 문서를 온라인으로 유지한다. 현재는 ECMAScript 2021이며 이 문서는 Living Standard로 주기적으로 갱신되는 듯하다. TC39이 표준을 제정하는 방식에 대해선 ECMAScript와 TC39 | ahn.heejong을 참고하라. 3. WHATWG 소개 The WHATWG was formed in response to the slow development of World Wide Web Consortium (W3C) Web standards (…) On 28 May 2019, the W3C announced that WHATWG would be the sole publisher of the HTML and DOM standards. WHATWG는 W3C이 제정하는 표준 중 HTML, DOM 표준을 독자적으로 제정하는 그룹이다. WHATWG는 주요 브라우저 벤더로 구성된다. 자세한 내용 WHATWG에서 제정한 HTML 5 스펙은 HTML 뿐만 아니라 - HTML5라고 해서 HTML만을 다루지는 않는다 - (현대) 웹 기술의 전반에 대해 표준화된 문서이다. 4. 번외: 구글은 왜 Chrome, V8을 오픈소스화했을까? [Chromium Release Announcement 발췌] With a richer set of APIs we can build more interesting apps allowing people to do more online. The more people do online, the more they can use our services. […] We believe that open source works not only because it allows people to join us and improve our products, but also (and more importantly) because it means other projects are able to use the code we’ve developed. [StackExchange 답변 중] they just needed the web as a whole to become more attractive. Their decision to open-source V8 led to one such effect: the NodeJS system was built on V8 … The web has become so attractive as a development environment that it is even displacing native apps, e.g. Electron is based on Chromium. 공유되는 일부 내용을 정리해보자면, 크롬을 통해 구글은 웹 시장을 키우고 싶었고, 오픈소스화(2008)로 웹의 기능을 늘리고 늘어난 기능이 널리 퍼지는 것을 유도했다(다른 크로미움 기반 브라우저를 통해). 오픈소스를 활용해 크롬을 개발했으며, V8기반의 NodeJS(2011)를 통해 Javascript 생태계가 커졌다. (숙련된 개발자가 많아질 수 있는 환경 조성) 출처: Why did Google make Chromium Open Source? | StackExchange TODO:Q. Netscape은 왜 넘겼을까? 돈이 안 돼서 그런걸까? 독자적인 기능으로 탑재했다면 더 성공할 수 있지 않았을까? Q. 굳이 오픈소스화 하지 않아도 경쟁을 통해 기능은 늘어났을 것이고 크롬 출시연도인 2009년에 이미 2위 브라우저로 시작해 2012년엔 1위 브라우저가 됐는데 오픈 소스화가 여기서 얼마나 큰 역할을 했는지는 잘 모르겠다. 좀 더 찾아봐야 할 듯.","link":"/js-license/"},{"title":"Promise.all은 Parallel로 실행되는가?","text":"Javascript의 모든 코드는 이벤트 루프에서 처리된다. 그리고 Node.js의 이벤트 루프 구현체로 libuv가 사용 된다는 사실은 널리 알려져 있다. 그런데 이상하지 않은가? Promise.all은 Parallel로 실행되는가?이벤트 루프 모델을 이해했다면 자바스크립트에 병렬 실행은 없다는 것을 이해했을 것이다. 파일, 네트워크 I/O는 자바스크립트 코드가 직접 처리하는 것이 아니고, 콜백은 이벤트 루프에 의해 호출되어 순차적으로 실행된다. 그럼, Promise.all은 어떤가? 음… 애초에 자바스크립트 코드가 병렬적으로 실행될 수 있는가? Promise.all을 잘 몰라서 생긴 일Promise.all은 Promise의 호출 순서와는 전혀 관계가 없다. Promise의 기본 동작을 하나도 건드리지 않는다. Promise.all이 제공하는 기능이란, 트랜잭션과 같이 하나라도 실패하면 catch 훅으로 넘어가게 하는 것이다. 1234567Promise.all([p1, p2, p3, p4, p5]) .then((values) =&gt; { console.log(values); }) .catch((error) =&gt; { console.log(error.message); }); 오히려 순차적으로 Promise를 실행하는 것이 더 특별하다.reduce를 사용해 iterable.reduce((p, fn) =&gt; p.then(fn), Promise.resolve())로 순차적으로 실행시킬 수 있다. (웬만하면 then으로 직접 잇겠지만.) Async Functions in Javascript 1를 참고해도 좋을 것 같다. 출처: How does Promise.all all works interanlly 출처: Is Node.js native Promise.all processing in parallel or sequentially? | StackOverFlow","link":"/js-promise-all/"},{"title":"NestJS를 사용해야 할 이유","text":"Express를 사용해야 할 이유 (1)을 쓰면서 NestJS는 굉장히 매력적인 대상으로 다가왔다. 이번 글에서는 NestJS를 사용해야 할 이유를 조금 더 자세히 정리하고, NestJS를 간략히 알아본다. NestJS를 사용해야 할 이유NestJs만의 장점이 뭐가 있을까? 생각보다 좀 있었는데, 아무래도 Rich Framework의 특징을 많이 갖고 있다. 장점 설명 CLI 생산성에 도움이 되는 유틸을 제공한다고 한다. 문서화 자세하고 유지보수가 잘 되는 문서를 갖고 있다고 한다. (타 프레임워크에 비해 기능이 훨씬 많은데도) 활성화된 개발 팀 이전 글에서 알아본 바로 이는 굉장히 특장점이다. (다른 언어에 비해 백엔드가 많이 약하다고 생각함) Nest 전용 모듈 Spring처럼 타 라이브러리를 쉽게 사용할 수 있게 전용 모듈을 개발했다. TypeORM, Mongoose, GraphQL, Logging, Validation, Caching, WebSockets 등의 모듈이 있다고 한다. MSA를 염두에 둔 설계 (이건 내가 MSA에 대한 이해가 적어서 얼마나 효과적일지 모르겠음.) Typescript 채택 (Typescript 경험이 일천해 얼마나 효과적일지 모르겠음.) 테스트 용이성 프레임워크 핵심 가치에 테스트 용이성이 있고 프레임워크에서 설계를 제공하므로, 다른 프레임워크에 독자적인 설계를 했을 때보다 테스트가 용이할 것으로 기대됨. NestJS를 사용하는 기업사용하는 기업 목록 중 SW적으로 큰 기업이 없는데 무슨 이유일지 모르겠다. 분명 시장의 선택은 합리적일텐데 정말 선택받지 못한거라면 중요한 문제가 있을 거라는 합리적인 의심을 해볼 만하다. NestJS의 핵심 패러다임Typescript를 지원하면서 OOP, FP, FRP(Functional Reactive) 요소를 조합한 백엔드 프레임워크이다. Typescript 사용은 강제가 아니다. Express, Fastify 를 기반해 개발됐고, NestJs가 윗단으로 추상화 계층을 제공하지만 바로 접근할 수도 있다. NestJs는 Node.js HTTP 프레임워크 추상화 계층을 구현해놓았기 때문에, Express, Fastify 이외에도 Adapter 패턴을 통해 인터페이스 구현체만 제공한다면 어떤 기술 위에서도 작동할 수 있다고 한다. 아마 Fastify로 이주할 때 개발해 놓은듯하다. @nestjs/platform-expres, @nestjs/platform-fastify 로 패키지가 분리돼있으니 참고해보면 재밌을 것 같다. 다만 NestJs에서 Express 생태계가 필요할 것 같지도 않고 Fastify가 훨씬 빠른데 굳이 Express를 기본값으로 해놓은 이유는 모르겠다. NestJs는 Typescript를 사용하지 않는다면 Babel이 필요하다고 하며 Node.js 10.13버전 미만으로는 지원하지 않는다. Typescript가 정확히 어떤 Javascript 버전까지 지원하는지 확인할 수 없었는데 조만간 Typescript를 학습하면서 정리해야겠다. 목표프론트 3대 프레임워크 덕분에 개발자 생산성이 향상됐고 빠르고 테스트 가능하고 확장성있는 프론트엔드 개발이 가능해졌는데, 그 외 좋은 라이브러리들이 많지만 애플리케이션 구조, 설계 측면의 문제를 해결하는 프로젝트는 없었다. (이건 Javascript 계열의 특징이라고 생각한다. 아마도 대규모로 개발하는 제품에 Javascript를 쓰지 않기 때문인 것으로 보인다. 요즘은 언어가 많이 좋아졌는데도 말이다.) NestJs는 애플리케이션 아키텍처를 제공한다(즉 개발자가 결정하는 게 아님). 테스트 가능하고, 확장성 있고, 느슨히 결합되고, 쉽게 유지보수 가능한 설계이다. Angular에서 영감을 받았다. (Angular는 강제성 있는 구조를 제공한다. React는 그 반대이고.) NestJS Docs Intro 요약문서의 내용을 요약했다. 프로젝트 제너레이터nest new {project_name} 을 입력하면 프로젝트 폴더가 생성된다. (npm i -g @nestjs/cli로 설치되는 CLI 유틸인듯) 기본으로 생성되는 프로젝트 구조는 아래와 같다. 도메인이나 레이어 별로 폴더가 나뉘진 않는 듯하다. src app.controller.ts app.controller.spec.ts app.module.ts app.service.ts main.ts 아래는 Express에서의 파일 역할 비교이다. file NestJS Express Controller app.controller.ts (사용자 나름) Service app.service.ts (사용자 나름) App app.module.ts App.js Index (진입점) main.ts index.js 재밌는 문서 구성아래는 몇 개의 하위 문서에 들어갔을 때 맨 위에 보이는 모식도 몇 개를 가져온 것이다. Spring에서도 제공하지 않는 모식도를 Nest에서 제공하는 게 재밌었는데, 개발진들이 정말 OOP를 좋아하는 것 같다고 느껴졌고 그래서 오브젝트 책을 보면서 같이 배우면 재밌을 것 같았고 Spring에 비해 Nest는 확실히 설계를 결정해주는 느낌이 들어서 자신감이 느껴졌고 그동안 설계를 정해준 프레임워크는 사용해 본적이 없었는데 받아들이기만 한다면 생산성도 꽤 좋아질 것 같다. 설계 수준 또한 오픈 소스로 개발되니 어느 정도 검증됐을 거라고 생각한다. 따라서 꽤 좋은 학습 경험을 주지 않을까 생각이 든다. 출처: NestJS Docs Why choose Nest.js as your backend framework? | selleo","link":"/jsb-1-nestjs/"},{"title":"리눅스 주요 디렉토리의 이름과 의미 정리 (계속 업데이트 예정)","text":"이 글은 리눅스를 처음 시작하는 경우 다른 OS의 디렉토리 구성과의 큰 차이로 인한 불편함을 줄이기 위한 큰 지도이다. 1. Home 디렉토리크게 2가지 Home 디렉토리가 있다. 이는 사용자에 따라 구분되는데, 일반 사용자: /home/{USER_NAME} 루트 사용자: /root 형태로 사용한다. 2. 시스템 디렉토리시스템 디렉토리란 사용자 디렉토리를 제외한 (거의) 모든 디렉토리를 말한다. 윈도우에서 Program Files 폴더가 시스템 디렉토리에 포함된다고 생각하면 이 정의에 동의할 수 있을 것이고, Windows, AppData 등의 폴더만 시스템 디렉토리라고 생각한다면 이 정의에 동의하긴 어려울 듯 하다. 일반적인 경로 구성 (ex) /usr/local/share/emacs /usr/local : 스코프 /share : 카테고리 /emacs : 응용 프로그램 경로 구성 요소 - 스코프스코프는 가장 상위 디렉토리로서 하위 디렉토리인 카테고리나 응용 프로그램이 실행되는 범위를 나타낸다. 다만 구분에 사용되는 명확한 기준은 없고 단지 /가 /usr보다 좀 더 운영 체제에 가까운 근본적인 수준이라는 느낌이라고 한다(출처: 리눅스 핵심 레퍼런스). 스코프의 종류: / : 리눅스 시스템 파일 /usr : 또 다른 리눅스 시스템 파일 Differences between /bin, /sbin, /usr/bin, /usr/sbin, /usr/local/bin, /usr/local/sbin 참고 /usr/local : 개인 컴퓨터에서 지역적으로 생성되는 시스템 파일 (ex) /usr/local/bin : 기본 프로그램이 아닌 경우 주로 여기에 설치된다. 경로 구성 요소 - 카테고리카테고리는 하위 디렉토리인 응용 프로그램의 목적을 나타낸다. 실행 파일: bin : 바이너리 sbin : 바이너리 (root 사용자 용이어서 root 권한이 필요하다.) lib : 바이너리에서 사용되는 라이브러리 Ubuntu 20.04의 경우 / 스코프의 bin, sbin, lib이 /usr 스코프로의 심볼릭 링크로 돼 있음 (어떤 배포판들이 또 이렇게 돼있는지는 확인 x) 설정 파일: etc : 시스템 설정 파일 init.d, rc.d : 부팅 설정 파일 문서: doc, info, man, share HW 관련: dev : Linux에서는 장치를 파일로 표현한다. media, mnt : Disk 마운트 지점 리눅스는 디렉토리 트리가 하나여야 하므로 mount를 통해 트리를 병합해야 함 [ 리눅스 마스터 ] 리눅스 디스크관리 - 마운트 (+실습, 명령어정리 ) Runtime: var : 클라이언트마다 다른 파일을 저장 Linux 기본 - /var 디렉토리의 이해 run : 실행 중인 프로세스의 ID를 담고 있는 PID 파일 운영체제 관련: boot ( /boot/vmlinuz ): 커널, 부팅 관련 파일 lost+found : 손상된 파일 등 추후 복구를 위한 임시 저장 경로 What is the purpose of the lost+found folder in Linux and Unix? proc : 현재 실행 중인 프로세스 정보. 저용량이며 실시간으로 반영한다. cat /proc/ioports : I/O HW 목록 표시 cat /proc/cpuinfo : 프로세서 정보. 코어 단위로 상세 정보를 출력 cat /proc/version : OS 버전 정보 표시 (ex: Linux version 4.19.128-microsoft-standard) cat /proc/uptime : ms 단위로 uptime 출력. (uptime 명령어가 더 낫다) ll /proc/{PID} : PID에 대응되는 프로세스의 정보. (많은데 잘 모르겠다) ll /proc/self : 현재 실행중인 프로세스로의 심볼릭 링크 (ex) 1261 이 글은 꾸준히 계속 업데이트될 예정입니다!","link":"/linux-dir/"},{"title":"1장: Reactor 패턴 (내용 검증 필요)","text":"1장은 Node.js에 대한 소개하는 챕터이다. 주의!해당 글의 내용은 부정확한 내용이 아주 많을 수 있습니다. 책의 설명이 추상적이고 OS 개념이 많이 필요하므로 추후 정리가 완료되는 경우 따로 표시하겠습니다. Node.js 철학 (Node Way)최소 기능: 기능 개수를 최소한되므로, 개발자, 사용자 모두에게 간단함 Node.js 자체 뿐만 아니라 node 기반 모듈을 설계할 때도 동일하게 적용 KISS 원칙: 부족하더라도 복잡함보다 단순함이 더 낫다 Reactor 패턴과 Node.js 이벤트 루프Reactor 패턴은 Node.js의 비동기 특성 - Node.js에서 여러 요청이 동시에 있는 경우는 항상 비동기 방식으로 작업을 처리한다 - 의 원인이자, 비동기 방식으로 작업을 처리하는 방법에 해당한다. Reactor 패턴을 배우기 전에, 동시성을 처리하는 2가지 방법에 대해서 알아보자. Blocking I/O : 느린 I/O를 기다리는 방식 많은 스레드 개수: 소켓의 데이터를 매번 기다리게 되면 각 연결 별로 스레드가 적어도 하나씩 돌아야 한다. 기다리는 시간에 타 사용자가 기다리지 않게 하기 위해서이다. 비효율적인 대기 시간: I/O가 CPU에 비해 매우 느리기 때문에 블로킹 API는 스레드의 유휴 시간이 처리 시간에 비해 압도적으로 길 수 밖에 없다. 스레드가 아무 일을 하지 않은 상태로 긴 시간 존재한다. 스레드의 비용: 스레드는 그 비용이 싸지 않다. 아주 많은 스레드가 있는 경우, Context Switching만 해도 비용이 매우 클 것이고, 적은 스레드가 있는 경우 사용자를 처리하지 못하므로 비즈니스적으로 비용이 매우 클 것이다. Non-blocking I/O: 비동기 API를 호출 시 바로 제어권을 반환(내부적으로 특정 상수를 반환)하여 CPU 유휴 시간을 최소화한다. Polling: 비효율적으로 I/O를 처리하는 방식으로, 리소스는 데이터가 없을 때 읽기 조작을 요청 받는 경우 EAGAIN을 반환하는데, 이 때문에 값이 필요한 입장에선 리소스를 계속 확인해야 한다. 이걸 BUSY_WAITING이라고 하는데, CPU를 계속 활용하므로 효율적이지 못하다. 동기 이벤트 디멀티플렉서: 논블로킹을 처리하는 효율적인 방법으로, 이벤트가 완료될 때마다 큐에 이벤트를 쌓아놓고 처리를 수행하는 객체. 이벤트가 없으면 Block 상태로 대기한다. 이벤트 통지자가 감시 대상 리소스의 자원이 읽기가 가능할 때(즉, 이벤트가 완료되었을 때) Demultiplexer에게 통지한다. (이벤트 통지자 역할로 IOCP, epoll/kqueue 등이 있는 것 같다.) Event가 발생하면 Event Demultiplexer가 깨어나 Queue에서 이벤트를 읽어들여 처리하면 됨. 이 시점에서 리소스의 I/O 작업은 (1)에서 이미 완료되어있으므로 동기식으로 처리하면 됨. 또한 처리 방식이 싱글 스레드이므로 공유 자원 문제도 존재하지 않는다. 리액터 패턴: 이벤트 디멀티플렉서 + 이벤트 루프 + 이벤트 큐 + 실행 환경(V8, 싱글 스레드!) 이벤트 디멀티플렉서는 I/O 처리가 끝나면 (완료된) 이벤트를 이벤트 큐에 넣어줌 이벤트 루프는 실행 환경 상에서 스택이 비는 경우(즉 모든 동기 코드가 실행이 끝났을 때 - 노드 환경에서 동기 코드는 얼마 없어서 최초의 동기 코드는 금방 끝나기 마련.), 이벤트 큐에서 이벤트를 꺼내어 실행 환경에 이벤트 핸들러를 올리고, 인자로 이벤트를 넘겨 수행함. 만약 async 내에 async가 있다면 해당 이벤트는 또 이 과정을 거침. 이벤트 디멀티플렉서의 구현체 libuv: 크로스 플랫폼으로(가상머신 느낌으로 각 OS에 대응되는 이벤트 통지자를 활용) 비동기 작업을 처리함. 단, libuv는 이벤트 디멀티플렉서 역할만 하는 게 아니라 이벤트 루프도 구현함. 참고: libuv에 이벤트루프가 포함돼있음: 더 정확한 이벤트 디멀티플렉서, 이벤트 루프 구현 상세에 관한 글, 영상 로우 레벨로 살펴보는 Node.js 이벤트 루프 | Evans Library Node.js 이벤트 루프, 타이머, process.nextTick() | Node.js (놀랍게도 이 문서가 더 어려운 것 같다…) 브라우저 환경에서의 이벤트 루프(자막 있음, 자세함!):Jake Archibald: In The Loop | JSConf.Asia 아마도 이벤트 루프에 대한 가장 유명하고, 쉬운 설명:What the heck is the event loop anyway? | JSConf EU","link":"/ndp-1-reactor-pattern/"},{"title":"리눅스 셸(bash) 기본 기능 소개","text":"이 글에서 소개할 내용은 굉장히 유용한 기능이라고 생각하며, 셸의 역할인 코드 실행 시의 입/출력에 대한 Proxy, Middleware 역할을 잘 활용하는 기능들이라고 생각한다. 이 글은 리눅스 핵심 레퍼런스의 일부를 참고해 작성하였다. 셸은 단순한 호출보다 훨씬 많은 것을 할 수 있다. 1. wildcard: 정규표현식과 유사한 검색을 수행한 결과를 명령의 입력으로 사용 1. 가장 기본이 되는 예제이름에 wildcard 검색을 수행해 결과를 명령의 입력으로 사용할 수 있다. 이름 일치 기준은 디렉토리이다. 즉 a*를 셸 명령어에 입력하게 되면, ./a로 시작하는 파일/디렉토리를 반환한다. 특정 상위 폴더나 하위 폴더를 대상으로 검색하고 싶은 경우 그에 맞는 상대 경로를 입력하면 된다. (ex 1) ls a* == ls aardvark adamantium apple (ex 2) ls githubblog/.* == ls githubblog/.git githubblog/.deploy_git, ... 위 사진은 기준 디렉토리이다. 여기서 g* 인 파일/디렉토리를 ls -alF의 매개변수로 주려고 한다. 위 사진은 명령의 결과물로, 실제로 잘 수행됨을 확인할 수 있다. 2.추가 옵션[문자들...] : 문자들 중 하나와 일치하는 경우. [aieou] : 모음 중 하나. 단 이렇게 찾으려면 파일/디렉토리 이름이 a, i, e, o, u 중 하나여야 한다. (즉 한 글자) [^문자들...], [!문자들...] : 명시된 문자 이외의 any 문자 [^aieou] : 자음 중 하나. 단 이렇게 찾으려면 파일/디렉토리 이름이 b, c, d, f, g, … 중 하나여야 한다. (즉 한 글자) ? : 임의의 한 문자. character 하나의 placeholder라고 생각하면 편리하다. [githubblo?] : githubblog가 있다면 일치한다. * : asterisk의 일반적인 의미처럼 아무거나. empty를 포함한 모든 string을 의미. 보통 조합할 때 필수적으로 사용된다. *[aioeu] : 모음으로 끝나는 경우 *[aioeu]* : 모음이 포함된 경우 [aioeu]* : 모음으로 시작하는 경우 2. 중괄호 확장: 단순히 가능한 모든 경우의 수를 입력으로 사용문자열 중간에서 사용되며 가능한 모든 경우의 수로 치환된 후 입력으로 사용된다. (ex) echo a{b,c,d}e{f,g,h} == echo abef abeg abeh acef aceg aceh adef adeg adeh 3. 변수: String 타입의 환경 변수 bash 프로파일 관리에 대해선 좀 더 나중에 다루려고 한다. Windows의 환경 변수와 cmd 환경 변수와 같은 2가지 변수가 있다. 모두 환경 변수이지만 그 범위가 다른데, cmd 환경 변수에 대응되는 Linux에서의 개념이 셸 변수이다. 단 아래의 방법으로 하면 해당 세션(셸)에서만 사용할 수 있으므로 일회성 변수로 생각하면 좋다. 쓰기 : MYVAR=string_value 읽기 : $MYVAR 만약 환경 변수로 저장하고 싶다면, export MYVAR=string_value와 같이 사용하면 된다. 환경 변수는 printenv 혹은 env 명령으로 확인할 수 있다. 기본으로 제공되는 환경 변수PATH : 바이너리 검색 경로의 목록. 콜론으로 구분. PWD : 현재 디렉토리 ( OLDPWD : 마지막으로 방문한 디렉토리 ) HOME : 홈 디렉토리 ( ex : /home/sb ) USER : 로그인명 ( sb ) 4. alias단순한 String 치환이다. 지정 : alias ll = &quot;ls -lG&quot;를 입력하면, 이후 셸에서 ll을 입력하면 ls -lG가 입력된다. 목록 확인 : alias만 입력하면 된다. 5. 입출력 redirection 아직 입출력을 파일을 통해 수행해본 적이 없어서 추후 적절한 예시를 추가하려고 한다. 이번 글에서는 개념적으로 그 사용법만 다룬다. 표준 입출력의 지점을 임의의 파일에 수행하게 한다. 입력을 파일의 내용으로 : command &lt; input_file 재미있게도 echo &lt; sample.txt 와 같이, echo 명령은 임의의 input 파일의 내용을 출력할 수 없는데, 표준 입출력에서 내용을 읽지 않기 때문이다. 출력을 파일로 : 새로운 파일로 작성 : command &gt; output_file 기존 파일에 이어 쓰기 : command &gt;&gt; output_file 오류의 경우 : command 2&gt; error_file 출력, 오류 모두 : command &gt;&amp; output_file 혹은 command &amp;&gt; output_file 출력, 오류 각각 : command &gt; output_file 2&gt; error_file 리눅스에서 표준 스트림은 3가지이며 자세한 내용은 위키 백과 (표준 스트림) 참고 6. Pipe 각 프로그램이 하나의 일을 잘 할 수 있게 만들 것. 새로운 일을 하려면, 새로운 기능들을 추가하기 위해 오래된 프로그램을 복잡하게 만들지 말고 새로 만들 것. 모든 프로그램 출력이 아직 잘 알려지지 않은 프로그램이라고 할지라도 다른 프로그램에 대한 입력이 될 수 있게 할 것. 무관한 정보로 출력을 채우지 말 것. 까다롭게 세로로 구분되거나 바이너리로 된 입력 형식은 피할 것. 대화식 입력을 고집하지 말 것. 소프트웨어를, 심지어는 운영 체제일지라도 이른 시기에 수주에 걸쳐 이상적으로 시도해가며 설계하고 만들 것. 어설픈 부분을 버리고 다시 만드는 것을 주저하지 말 것. 프로그래밍 작업을 가볍게 하기 위해, 심지어 우회하는 방법으로 도구를 만들고 바로 버릴지라도 어설픈 도움 보다는 도구 사용을 선호할 것. 출처: 위키 백과 (유닉스 철학) Pipe 연산자는 유닉스 철학을 구현하는 도구 중 하나로, 이 중 2번 규칙을 지키는 도구로 사용된다. (ex) who | sort | awk '{print $1}' | less pipe 연산자의 효과를 제대로 소개하는 예제를 만들기엔 아직 아는 명령어가 극히 적어서 추후 제대로 소개하고자 한다. 해당 소개 글이 작성될 경우 이 글에서 링크를 제공하도록 하겠다. 7. 평가식평가식이란 그 내용이 코드로 해석되는 영역을 말한다. 셸에서의 평가식은 해당 평가식을 셸에서 따로 실행시켰을 때의 결과를 반환하는 형태를 갖는다. 이 평가식의 문법은 크게 두 가지가 있는데, backtick : echo This year is ``date +%Y\\`` This year is 2021 $() : echo Next year is $(expr $(date +%Y) + 1) Next year is 2022 평가식으로 (5)에서 실패했던 echo &lt; sample.txt를 평가식으로는 실행할 수 있다: echo $(cat sample.txt) (`을 사용해도 된다.) 8. 작업 제어셸에서 수행되는 프로그램은 대개 포그라운드로 실행된다. 즉 사용자와의 인터렉션이 블로킹되는데 셸에서 프로그램을 실행할 때 백그라운드로도 실행시킬 수 있다. 또한 포그라운드와 백그라운드를 넘나들 수 있으며 작업을 정지하고 다시 실행할 수도 있으며 셸 마저 정지할 수도 있다. 백그라운드로 작업 실행 : command ... &amp; (&amp;가 핵심이다.) 포그라운드 작업 정지 : Ctrl + Z 백그라운드에서 작업 재개 : `bg {id}`` 포그라운드로 작업을 가져와 실행 : fg {id} 현재 수행 중인 작업의 목록 조회 : jobs 현재 셸 정지 : suspend (현재 실행 중인 셸이 2개 이상이어야 호출 가능하다.) 9. 여러 셸 동시 사용screen은 내장 기능이어서 dependency가 추가로 필요하지 않아 사용에 제약이 없지만 그 기능이 적고 불편하다. tmux가 많이 사용되며 기본으로 설치되는 경우도 있으나(WSL Ubuntu에는 기본으로 설치돼있다.) 둘 모두 사용해본 적이 없어 추후에 다루도록 한다. TODO : 입출력 Redirection 적극적으로 활용해보기 (특히 알고리즘 테스트 케이스 수행 시) screen/tmux 모두 사용해보고 비교하기 리눅스 명령어 더 공부하고, pipe로 효과적인 예 만들기","link":"/linux-shell-1/"},{"title":"2장 (1&#x2F;3): CPS 패턴","text":"이 글은 CPS 패턴과 CPS가 Node.js에서 어떻게 사용되고, 어떤 점을 주의해야 하는지 다룬다. 1. CPS 패턴Node.js는 1장에서 살펴봤듯 비동기 특성을 가지며, 따라서 Node.js 앱은 대부분의 일을 비동기로 처리할 수 밖에 없다. 비동기를 처리하는 방법 중 CPS, Continous Passing Style을 소개한다. CPS: 비동기 API를 사용할 때, 콜백 함수를 인자로 넘기는 패턴이다. 왜 사용하는가: 비동기 API는 return을 할 수 없는데, 함수의 실행이 끝나기 전에 제어권이 넘어가기 때문이다. 이를 해결하기 위해선 결과를 다른 함수에 넘기면 된다. 장점: 간단하고 효과적이다. 단점: 호출 깊이가 깊어지면 가독성이 감소된다. Callback Hell이라고 불린다. 2. Node.js에서의 CPS 패턴Node.js는 CPS 패턴을 사용할 때 일관된 규칙을 따라야 한다. argument 순서에 관한 규칙: (...params, callback) 과 같이, callback 함수를 마지막 인자로 넘겨야 한다. callback 함수의 argument에 관한 규칙: (err, ...args) 와 같이, err가 첫 인자여야 한다. err 인자의 경우, 항상 Error() 객체여야 한다. (이 부분은 잘 지켜지지 않는 듯 하다.) 3. CPS 패턴의 콜 스택Node.js에서 비동기 API를 호출하는 경우, callback 함수는 프로그래머가 예상한 호출 순서로 구성된 스택을 갖지 않는다. 비동기 API가 완료됐을 때, 이벤트 루프에 의해 단일 함수로 Queue에 쌓인 후 다른 타이밍에 실행되기 때문에 새로운 스택에서 실행된다. 비동기 함수에서 예외를 던지면, Error를 반환하며 프로세스가 종료된다. 12345678910111213141516171819202122const fs = require('fs');const readJsonThrows = (filename, cb) =&gt; { try { fs.readFile(filename, 'utf8', (err, data) =&gt; { if (err) return cb(err); cb(null, JSON.parse(data)); // cb이 없거나, data가 불량인 경우 exception 발생 가능. // 콜백 함수 내에서 try-catch하지 않는 경우 프로세스가 죽는다. }); } catch (err) { // 여기서도 catch할 수 없음. 호출 스택은 fs.readFile에서 끝나고, // cb은 별개의 새 스택에서 실행되기 때문 }};// 만약 JSON.parse에서 오류나는 경우, 프로세스가 종료된다.readJsonThrows('C./test.json', (f) =&gt; f);/* SyntaxError: Unexcepted end of JSON input at JSON.parse at FSReqCallback.readFileAfterClose (internal/...)*/ 4. Node.js에서 비동기를 처리할 때 절대 하지 말아야 할 점들1. 결괏값을 동기, 비동기 2가지 방식으로 전달하지 않는다. 결괏값이 비동기일것을 기대하고 이벤트 리스너를 등록할 때, 동기로 결괏값이 제공되는 경우 이벤트 리스너가 동작하지 않는다. 동기 반환값을 비동기화 한다. setTimeout, setImmediate, nextTick, Promise 등이 가능하다. 2. Callback 함수를 argument로 받는 동기 함수를 작성하지 않는다. 동기 API는 바로 결괏값을 받는 형태로 코드를 작성하면 된다.","link":"/ndp-2-cps/"},{"title":"2장 (3&#x2F;3): Node.js의 Observer Pattern","text":"이 글은 Node.js에서 자주 사용되는 Observer Pattern에 대해 소개한다. 리액터 패턴, CPS 패턴에 대한 지식을 전제로 작성했으니 참고바란다. 1. Observer Pattern의 정의 Node.js에서 이벤트는 핵심 중 하나라고 한다. Node.js 코어 모듈과 오픈 소스를 사용하는데도 필수적인 조건이라고 한다. Observer Pattern은 Subject와 Listener 라는 역할로 한 쪽은 등록을, 한 쪽은 통지를 하는 관계이다. Subject는 이벤트를 발생시키는 주체로, 스스로 무슨 행위를 할 때, Listener에게 통지를 해야 한다. Listener는 특정 Subject 객체에 본인의 참조를 등록한다. subject.addListener(this)와 같이 수행한다. foreach (listener : listeners) listener.notify(); 와 같이 Listener에게 이벤트 발생을 알린다. 옵서버 패턴 | Wiki 백과 참고. Observer Pattern이 Callback 보다 나은 점이 뭘까? 기능 Observer Pattern Continuous Passing Style 다중 리스너 지원 Yes No 핸들러 사용 횟수 여러 번(or 주기적으로) 발생하는 경우 한 번 발생하는 경우 핸들러 함수 제약 없음. onError, onSuccess 로 관심사 분리하므로. 한 함수 (err, data)=&gt; { /* ... */ }로 두 상태 모두 처리 핸들러 등록 시점 아무 때나 함수 실행 시점에 매개변수로 전달 2. Event EmitterNode.js는 Event Emitter라는 미리 구현된 객체를 코어 모듈(events)로 포함하고 있다. 이 객체는 emit, on, once, removeListener 로 구성된 총 4개의 메소드를 갖고 있다. 아래는 각 메소드의 사용 예시이다. CodeSandBox가 Node.js를 Beta로 지원하고 있으므로 출력이 정상적이지 않을 수 있습니다. 왼쪽의 탭을 드래그해 코드를 확인해주세요. 아래는 File을 읽는 예제이다. 3. Event Emitter 에서의 예외 처리Event Emitter에서도 비동기 이벤트의 경우, CPS와 마찬가지로 예외가 발생하는 경우 기존 스택을 잃기 때문에 (리액터 패턴 참고) try-catch로 무조건 예외를 처리하여야 한다. 이후 error 이벤트를 발생시켜 리스너들에게 전달함이 일반적이다. 4. Event Emitter 상속하기아래와 같이 EventEmitter를 상속하여 인스턴스에 대해 .on을 붙이는 등의 작업을 할 수도 있다. 책에서는 일반적인 패턴이라고 하지만, emit 메소드까지 의도치 않게 Public API가 되기 때문에 추천하는 방식은 아니다. 위임으로 on, once, removeListener를 따로 API로 내보내는 게 맞다고 생각한다. 123456class FindPattern extends EventEmitter { //...}const findPattern = new FindPattern(/hello \\w+/g);findPattern.on(/* ... */); 5. 동기, 비동기 이벤트 별 리스너 등록 시점이벤트를 동기적으로 발생시키려면, 리스너 등록을 이벤트 발생 이전 시점에 완료하여야 한다. 이벤트를 비동기적으로 발생시키는 경우, 리스너를 동기적으로만 등록한다면 시점이 자유롭다. (리액터 패턴 참고.)","link":"/ndp-2-event-emitter/"},{"title":"2장 (2&#x2F;3): Node.js의 모듈 시스템","text":"이 글은 Node.js의 모듈 시스템에 대해 소개한다. 1. 모듈 시스템의 필요성과 Javascript의 방식모듈 시스템은 프로그램의 구성 요소들 간의 역할을 분리하고, 의존 관계와 구현 상세를 격리하는데 필수적이다. 모듈 시스템의 문법으로 보면, 소스 파일간의 import, export를 하는 것인데, 개념 상 Java의 접근 제한자 - private, protected, public - 도 모듈의 역할 중 일부를 수행 한다고 할 수 있다. Javascript 모듈 시스템으로는 대표적으로 ESM, CommonJs 라는 두 개의 기술이 있는데, 현재의 Node.js는 ESM, CommonJs를 모두 지원한다. 종류 ESM CommonJS 제정 시기 ES6에 제정됨 ESM 이전의 대표적인 비표준 문법(Node 기준) import / export require / module.export Node.js 지원 여부 Yes Yes Browser 지원 여부 최신 브라우저에서 지원 CommonJs.js 로딩 필요 자세한 역사와 기타 모듈 시스템의 종류는 JavaScript 표준을 위한 움직임: CommonJS와 AMD | Naver D2를 참고. 2. Revealing Module PatternJavascript에는 접근 제한자가 없다. 접근을 원천적으로 제한하는 방법 중, 공개할 부분만 객체로 담아 내보내는 패턴이 있다. Private 변수는 클로저를 통해 접근할 수 있으므로, 꽤 괜찮은 방법이다. Revealing Module 패턴을 구현하는 방법은 대표적으로 IIFE(즉시 실행 함수 표현식)가 있다. IIFE는 익명 함수를 ()로 감싼 후 즉시 실행하는 함수 호출 방식이다. 123456789101112const module = (() =&gt; { const privateFoo = () =&gt; { /* private functionality */ }; let privateCounter = 0; const increase = () =&gt; ++privateCounter; const decrease = () =&gt; --privateCounter; // 이 객체를 반환하므로, 외부에선 privateFoo, Bar에 접근할 수 없다. return { increase, decrease };})(); // 즉시 실행하여, { increase, decrease } 객체가 반환된다. 3. CommonJs의 require 방식에 대해CommonJs는 const moduleA = require('./moduleA');와 같이 모듈을 로딩하는 문법을 제공한다. require는 동기로 작동하고, 한 번 로딩한 모듈은 캐시된다. 내보낼 때에는 각 모듈별로 제공되는 exports 객체에 필드를 할당하는 방식으로 진행한다. 모듈은 캐싱되므로 항상 동일한 객체를 반환한다. 아래는 require의 수도 코드이다. 1234567891011121314151617181920212223242526272829const require = (modulePath) =&gt; { // path를 가져오고, unique한 id로 활용한다. const id = require.resolveAbsolutePath( modulePath, ); // 캐시된 모듈은 캐시를 반환한다. if (require.cache[id]) return require.cache[id].exports; // 처음 로딩하는 경우 새 exports 객체가 필요하다. const module = { exports: {}, id, }; // 객체는 캐시한다. require.cache[id] = module; // 이 함수가 소스 코드를 읽어 exports 객체에 export 내용들을 할당한다. readFileAndEvaluate(id, module, require); return module.exports;};require.cache = {};require.resolveAbsolutePath = (modulePath) =&gt; { /* implementation */}; 어느 범위까지 같은 인스턴스가 반환될까? 같은 패키지로 빌드된다면 하나의 인스턴스를 공유할 것이다. package.json별로 독립적으로 dependency를 관리하기 때문에, 각 패키지간에 제 3의 모듈의 객체를 주고 받는 경우, 해당 객체는 버전 불일치가 있을 수 있다. A Simple Explanation | Medium (EN) 4. 비동기 모듈 초기화비동기로 객체를 초기화할 순 없다. require 함수가 동기로 작동하기 때문인데, 아무래도 initialize와 같은 메소드를 호출하는 형태로 비동기 API를 만들어서 활용하는 수 밖에 없을 듯하다. 관련 스택 오버 플로우 참고. 5. 순환 참조가 있는 경우Node.js 환경에서 순환 참조를 하는 경우 한 모듈이 먼저 로딩되기 때문에, 동기로 로딩하는 경우, 한 쪽에서는 null, 한 쪽에서는 정상 로딩이 될 수 밖에 없다. 아니면 명확한 순서를 지정해준다면 해결할 수도 있겠지만(A[A.B = null]-&gt;B[B.A = A]-&gt;[A.B = B]), 순서를 명시하는 API가 따로 있는지 잘 모르겠다. 한 쪽에서 느린 초기화를 진행한다. (Lazy-Init) - 순서 정하기와 사실상 동일함. 순환 참조 관계에 있는 두 객체를 제 3의 객체에 의존하도록 한다. 관련 스택 오버 플로우 - 이 부분은 잘 이해하지 못 했다. 어떻게 export 해야 좋은 모듈일까?1. Substack 패턴모듈의 기능을 객체가 아닌 함수 단위로 노출한다. 진입점이자 주가 되는 함수를 module.exports로 내보내는데, 따라서 const logger = require('./logger')와 같이 바로 사용할 수 있는 함수가 된다. 또한, logger.verbose(msg); 와 같이 서브 함수들도 내보내, 사용하는 입장에서 기능의 중요도를 쉽게 파악할 수 있게 한다. 1234module.exports = mainFn;exports.subFn1 = subFn1;// 2...N-1exports.subFnN = subFnN; (ex) 1234567// 메인 함수module.exports = (msg) =&gt; console.log(`${this.name} ${msg}`);// 서브 함수 1exports.verbose = (msg) =&gt; console.log(`[verbose] ${this.name} ${msg}`); 2. 생성자 내보내기prototype 기반으로 생성자를 만들거나, ES6 Class를 활용하여 생성자를 만들어, 생성자를 내보낸다. 사용하는 입장에선 객체의 기능을 확장할 수도 있고, 쉽게 인스턴스를 생성할 수도 있고, 사용하기도 깔끔한 방법이다. 12345678910111213module.exports = class Logger { constructor(name) { // implementation } log(msg) { console.log(`${this.name} ${msg}`); } verbose(msg) { console.log(`[verbose] ${this.name} ${msg}`); }}; 3. 인스턴스 내보내기생성자 내보내기와 거의 같지만, 싱글톤이 자동으로 구현되는 셈이므로 쉽게 활용하기 좋다. 123456789101112131415class Logger { constructor(name) { // implementation } log(msg) { console.log(`${this.name} ${msg}`); } verbose(msg) { console.log(`[verbose] ${this.name} ${msg}`); }}module.exports = new Logger('App');","link":"/ndp-2-module/"},{"title":"3장: CPS 패턴 사용 시의 Tip","text":"Node.js 환경에서 CPS 패턴을 사용할 때 시도할 만한 Tip들을 정리했다. 1. Callback Hell을 조금 해결하는 방법 본인은 Promise 세대여서 Callback Hell을 제대로 경험해 본 적이 없고, 웬만한 개발 환경이라면 Callback Hell을 겪기 어려울 것으로 예상돼 짧게 요약했다. 들여 쓰기 때문에 가독성이 매우 떨어지게 되고, 변수 이름도 중첩되는 문제가 있다. 만약 Blocking API를 사용해 동일한 내용을 구현했다면 잘 못 이해할 가능성은 거의 없을 것이다. Pattern: 중첩 수준을 낮게 유지하기 위해, else 문을 사용하지 않는다. 인라인 함수의 이름을 지정하면, 함수 이름을 통해 더 쉽게 디버깅이 가능하다. 함수를 쪼갠다. 자주 하는 실수: Callback을 호출한 뒤에도 함수는 계속 실행됨을 잊는다. 12if (err) callback(err);// 여기서도 함수는 계속 실행된다. return callback(err) 혹은 return을 callback 호출 이후 수행하여 함수 실행을 종료한다. 2. 순차적으로 실행시키는 방법Callback Hell을 겪지 않고 비동기 API를 순차적으로 실행하는 방법: 재귀 함수로 실행한다. 재밌는 점은, StackOverFlow가 날 일은 없다는 점이다. 비동기 함수여서 매 번 스택이 초기화되니까. 123456789101112131415161718const length = N;const tasks = [ /* ... */];const data = [ /* ... */];const callback = (f) =&gt; f;const iterate = (idx) =&gt; { if (idx === length) return callback(); const task = tasks[idx]; task(data[idx], (err) =&gt; { if (err) return callback(err); iterate(idx + 1); });};iterate(0); // Callback이 재귀적으로 수행돼, N 만큼 수행된다. 이 방식의 한계: 실행될 작업의 숫자를 알아야 한다. 3. JS 경쟁 조건 해결하기 Javascript는 단일 스레드로 실행된다. 리소스 동기화는 필요 없지만, 비동기 API 타이밍 문제는 아직 남아있다. Javascript 역시 호출 시점과 I/O 수행 시점 차이로 중복 작업 등의 예기치 않은 동작을 할 수 있다. 상호 배제로 해결 가능하다. 12345678910// 실행 중인 job을 등록한다. 공유 리소스 동기화는 필요 없다.const jobs = new Map();const fn = (id, data, callback) =&gt; { // 이 코드로 타이밍 문제를 해결할 수 있다. if (jobs.has(id)) return process.nextTick(callback); jobs.set(id, true); // 정상 분기.}; 4. 동시에 수행되는 작업 개수 제한 하기한 번에 너무 많은 파일을 열려고 하는 등의 경우 리소스 부족으로 뻗어버릴 수 있다. 동시에 실행하는 작업의 수를 제한해 이를 상황을 방지하는 아이디어를 소개한다. 알고리즘: 처음에 동시 실행 제한 개수만큼의 작업을 실행 각 작업이 끝날 때, 동시 실행 제한 개수 - 현재 실행 개수 만큼의 작업을 실행 1234567891011121314151617181920212223242526const tasks = [ /* ... */];const limit = 2; // 동시 실행 제한 개수let running = 0, completed = 0, idx = 0;const next = () =&gt; { // 여유 작업 개수만큼 반복 while (running &lt; limit &amp;&amp; idx &lt; tasks.length) { const task = tasks[idx++]; task(() =&gt; { // 새 작업을 할 수 없음 if (completed === tasks.length) return finish(); completed++; running--; next(); // 새 작업을 할 여유가 있음 }); running++; }};// 동시 실행 제한 개수를 채우며 계속 실행함.next(); 큐로 구현하는 방법: 12345678910111213141516171819202122232425262728293031// Queue로 구현하는 방식// 로직은 같은데 Queue를 사용하는 것만 다르다.class TaskQueue { constructor(limit) { this.limit = limit; this.running = 0; this.queue = []; } // task :: callback =&gt; void; (must call callback) // task를 tasks에서 가져오는 게 아니라, Queue에 넣은 것이 나온다. // =&gt; 새 작업을 큐에 동적으로 추가할 수 있다. pushTask(task) { this.queue.push(task); this.next(); } next() { while ( this.running &lt; this.limit &amp;&amp; this.queue.length ) { const task = this.queue.shift(); task(() =&gt; { this.running--; this.next(); }); this.running++; } }} 5. Async 라이브러리 사용복잡한 비동기 제어 흐름을 선언적인 방식으로 처리할 수 있게 헬퍼 함수들을 제공하는 라이브러리이다. 순차적인 반복 제한된 동시 실행 등을 헬퍼 함수를 통해 쉽게 구현 가능하다. CPS 패턴은 주로 사용할 것 같진 않아 따로 정리하진 않았다. TODO: 여러 번 이해하여 좋은 예제를 만들어 이 글 내용 보강하기","link":"/ndp-3-cps-tips/"},{"title":"5장 Stream API (1&#x2F;3) - 스트림 개요 및 Readable Stream","text":"이 글은 Node.js 디자인 패턴 CH 05 스트림 코딩의 일부를 참고해서 작성하였다. 이번 글은 Stream API에 대해 깊이 다루기보다 스트림 자체에 대해 다룬다. 스트림 개요스트림은 파일을 버퍼 단위로 옮겨서 전부 옮길 때까지 기다린 후 처리하기보다 매 버퍼 단위로 전송하는 방식이다. 스트림은 본질적으로 비동기 패러다임으로, 기다린 후 처리하는 Sync 방식에 대비된다. 물론 fs.readFile 역시 Node.js 런타임에서 I/O를 처리해주니 스레드가 Block 되진 않겠지만, 애초에 I/O 수준에서도 기다릴 일이 없게 하는 것이 처리량에서 우위이지 않을까? (처리량에서 정말 우위일지는 잘 모르겠다. 스트리밍 오버헤드에 대해 공부해본 적이 없기 때문.) 스트림의 공간 효율성스트림은 메모리에 파일의 전체 내용을 올리지 않고 버퍼의 크기만큼만 메모리를 할당하기 때문에 공간 효율적이다. 더 좋은 점은 파일의 크기에 상관 없이 일정한 양의 메모리를 점유한다는 점이다. 이것과 별개로 V8 엔진은 32bit 기준 ~1GB, 64bit 기준 ~1.7GB 정도의 메모리만 사용하도록 설정돼있어(더 높이려면 빌드해야 함.) 파일이 큰 경우 전체 파일을 한 번에 메모리에 올릴 수 없음. 공간 비효율적인 파일 압축 코드 (ex: example.tar -&gt; example.tar.gz) 123456789101112const fs = require('fs');const zlib = require('zlib');const file = process.argv[2];fs.readFile(file, (err, buffer) =&gt; { zlib.gzip(buffer, (err, buffer) =&gt; { fs.writeFile(file + '.gz', buffer, err =&gt; { console.log('File successfully compressed'); }); });}); 공간 효율적인 파일 압축 코드 (Stream API) 1234567891011const fs = require('fs');const zlib = require('zlib');const file = process.argv[2];// 파일을 읽는데에 buffer 크기만큼만 메모리를 점유하기 때문에 공간 효율적// pipe 체이닝으로 각 chunk에 대해 이런 저런 처리를 할 수 있음.fs.createReadStream(file) .pipe(zlib.createGzip()) .pipe(fs.createWriteStream(file + '.gz')) .on('finish', () =&gt; console.log('File successfully compressed')); 참고로 gzip이 어떻게 스트림에 대해 동작하는지 궁금하다면 아래 글들을 참고해보면 좋을 것 같다. How is it possible to GZIP a stream before the entire contents are known? | StackOverFlow How does gzip compression rate change when streaming data? | StackOverFlow 스트림의 시간 효율성Stream은 TTFB(Time to First Byte)에 강점이 있는데, TTFB는 파일의 크기에 비례하여 빠를 수 밖에 없다. 파일의 크기가 클 수록 읽는 데 대기시간이 필요하지만 Stream은 곧바로 응답을 보내기 시작하기 때문이다. 웹에서 TTFB는 매우 중요하다. 자세한 건 Next.js의 재밌는 이슈(Stream rendering to reduce TTFB and CPU load) 참고. 파일을 단위로 전송하는 Server-Client 모델다음의 사이클을 단 1회 거치게 된다: read &gt; compress &gt; send &gt; receive &gt; decompress &gt; write chunk 단위로 전송하는 Server-Client 모델위의 사이클을 매 chunk마다 거치게 되므로 파이프라이닝과 같은 형태로 병렬 처리가 가능하다. 물론 chunk의 크기마다 다르겠지만 각 단계를 거치는 만큼 오버헤드가 있을 것이다. (HTTP header 등. 이 부분에 대해선 잘 알지 못한다.) Node.js 동시성을 활용하는 것이므로 순서를 맞춰줘야 하는데 Stream API가 알아서 처리한다고 한다. 아래는 파일을 전송하는 스트림 예제 코드이다. client: 파일을 받아 디스크에 쓰는 역할 123456789101112131415161718const http = require('http');const fs = require('fs');const zlib = require('zlib');const server = http.createServer((req, res) =&gt; { const filename = req.headers.filename; console.log('File request received: ' + filename); req .pipe(zlib.createGunzip()) .pipe(fs.createWriteStream(filename)) .on('finish', () =&gt; { res.writeHead(201 /* CREATED */, {'Content-Type': 'text/plain'}); res.end('That\\'s it\\n'); console.log(`File saved: ${filename}`); });});server.listen(3000, () =&gt; console.log('Listening')); server: 파일을 읽고 전송하는 역할 12345678910111213141516171819202122232425262728293031const fs = require('fs');const zlib = require('zlib');const http = require('http');const path = require('path');const file = process.argv[2];const server = process.argv[3];const options = { hostname: server, port: 3000, path: '/', method: 'PUT', headers: { filename: path.basename(file), 'Content-Type': 'application/octet-stream', 'Content-Encoding': 'gzip' }};const req = http.request(options, res =&gt; { console.log('Server response: ' + res.statusCode);});fs.createReadStream(file) .pipe(zlib.createGzip()) .pipe(req) .on('finish', () =&gt; { console.log('File successfully sent'); }); 스트림의 문제 해결력스트림은 Composition으로 문제 해결을 한다. Express Middleware와 같이 마음껏 파이프라인을 만들어낼 수 있다. 파이프라인은 각 기능 간에 결합이 없기 때문에 항상 1차원으로 코드가 표현된다. (분기가 없다는 게 아니라 가독성이 좋다는 것.) 선언형으로 프로그래밍하기 수월하다. 선언형 패러다임은 코드를 요약해서 바라볼 수 있기 때문에 쉽게 이해하기 좋다. 스트림을 기반으로 비동기 이벤트를 처리하는 패러다임을 Reactive라고 하고 이를 위한 RxJS가 있다. (ex) 암호화 기능 추가 12345// 복호화.pipe(crypto.createDecipheriv(&quot;aes-192-gcm&quot;, &quot;a_shared_secret&quot;))// 암호화.pipe(crypto.createCipheriv(&quot;aes-192-gcm&quot;, &quot;a_shared_secret&quot;)) Node.js에서 지원하는 스트림Node.js가 지원하는 스트림은 EventEmitter 객체를 상속하며 binary, 문자열 뿐만 아니라 거의 모든 Javascript의 값을 읽을 수 있다. 이러한 스트림에는 크게 네 종류가 있는데 이번 글에서는 (글이 길어지는 관계로) Readable만 다룬다. Readable, Writable, Duplex, Transform 1. ReadableReadable 스트림은 외부에서 읽기 위한 스트림으로, 자신이 가진 값을 chunk로 써서 내보내는 역할이다. 사용 예: readable 이벤트에 listener를 등록하고 이벤트 발생 시 버퍼에 있는 내용을 모두 읽기 API로는 아래의 함수가 있다. readable.read([size]) // read는 동기 함수이다. (ex) 표준 입력(stdin) 받아서 표준 출력(console.log, stdout.write)하기 1234567891011process.stdin .on('readable', () =&gt; { let chunk; console.log('New data available'); while((chunk = process.stdin.read()) !== null) { console.log( `Chunk read: (${chunk.length}) &quot;${chunk.toString()}&quot;` ); } }) .on('end', () =&gt; process.stdout.write('End of stream')); Stream v1, v2에 따라 non-flowing mode, flowing mode 로 나뉘는데 어차피 v1은 사용되지 않으므로 설명을 생략한다. ReadableStream을 하나 새로 만드는 예제지금까지는 fs, http의 스트림을 그대로 사용했지만 직접 ReadableStream을 만들어 활용할 수도 있다. stream.Readable을 상속해 abstract function인 _read([size])(public 인터페이스인 read와 헷갈리면 안 된다)를 구현하면 ReadableStream 객체를 하나 만들 수 있다. 구현을 위해 push(data[, encoding]) 함수를 호출해 내부 버퍼에 값을 쓸 수 있다. 12345678910111213141516171819202122232425262728293031const stream = require('stream');const Chance = require('chance');const chance = new Chance();// [1] 생성class RandomStream extends stream.Readable { constructor(options) { super(options); } _read(size) { const chunk = chance.string(); console.log(`Pushing chunk of size: ${chunk.length}`); this.push(chunk, 'utf8'); if(chance.bool({likelihood: 5})) { this.push(null); } }}// [2] 사용const RandomStream = require('./randomStream');const randomStream = new RandomStream();randomStream.on('readable', () =&gt; { let chunk; while((chunk = randomStream.read()) !== null) { console.log(`Chunk received: ${chunk.toString()}`); }}); TODO나머지 스트림 종류 다루기 백 프래셔 스트림 기반 비동기 제어 Pipe Composition 멀티 플렉싱, 디멀티 플렉싱","link":"/ndp-5-stream-1/"},{"title":"5장 Stream API (2&#x2F;3) - Node.js의 4가지 스트림 소개와 사용법","text":"이 글은 Node.js 디자인 패턴 CH 05 스트림 코딩의 일부를 참고해서 작성하였으며, Node.js에서 코어 모듈로 제공하는 Stream 4종류를 다룬다. Node.js에서의 스트림 자체에 대해서는 5장 Stream API (1/3) - 스트림 개요 및 Readable Stream를 참고하라. Node.js 스트림 객체Node.js에서는 4가지의 추상 스트림 클래스를 제공하여 쉽게 스트림을 구현할 수 있게 한다. 이 클래스들은 core 모듈에서 제공하므로 추가 의존성이 필요하지 않다. Name 목적 dataSource 가능 stream.Readable 외부 데이터 읽기 (dataSource에서 꺼내는 형태) True stream.Writable 내부 데이터 외부로 전송하기 (dataSource로 써주는 형태) False stream.Duplex Readable + Writable 스트림. True stream.Transform 외부 데이터 읽기 =&gt; 데이터 변조하기 =&gt; 외부로 전송하기 True Node.js의 두 버전의 스트림 APINode.js에는 두 가지의 Stream API가 있다. API Version Name Event Name Description Stream v1 Flowing Mode on('data') 무조건 해당 데이터를 처리해야 함. 버퍼 크기 등의 문제로 처리하지 못 하는 경우 해당 데이터를 되살릴 방법이 없음. Stream v2 Non-Flowing Mode on('readable') 곧바로 데이터를 처리하지 않아도 됨. 백 프래셔를 지원함. Back Pressure: Event 송신자의 처리량이 Event를 수신하는 측의 처리량을 넘기는 경우 송신자의 전송 속도를 줄여야 하는 경우가 생기는 데 이를 해결하는 메커니즘을 Back Pressure라고 한다. 송신자-수신자 처리량 차이 발생 오류 백프래셔 필요 송신자 전송량 &lt; 수신자 처리량 없음 False 송신자 전송량 &gt; 수신자 처리량 처리하지 못하는 데이터에 대한 정의되지 않은 동작 등 손실 발생 가능 True Stream v2의 백 프래셔:Node.js의 버퍼가 알아서 버퍼링을 해주며, 버퍼 한계치를 넘으면 OS에서 패킷을 drop시켜 sender 입장에서 전송 속도를 늦추게 함. 이 기능을 자동으로 지원. (v1도 가능하다고 함. 다만 더 어렵다고 함.) 출처: What are the differences between readable and data event of process.stdin stream? 추가 참고: [RxJava2]Flowable에서의 Backpressure [RxJava2]Backpressure와 함께 Flowable를 만들어 보자 1. Readable 스트림Readable 스트림은 데이터를 읽어들이는 게 목적이다. 1-1. 사용 예시:stream.read() 함수를 사용하면 chunk를 반환한다. 1234567891011const RandomStream = require(&quot;./randomStream&quot;);const randomStream = new RandomStream();// readable 스트림:// stream.read() 로 내용을 읽는 것을 의미한다.randomStream.on(&quot;readable&quot;, () =&gt; { let chunk; while ((chunk = randomStream.read()) !== null) { console.log(`Chunk received: ${chunk.toString()}`); }}); 1-2. Readable 구현 코드Readable Stream은 _read 함수를 구현하면 된다. 123456789101112131415161718192021222324252627282930const stream = require('stream'); // 코어 모듈 (stream)const chance = new require('chance')(); // 랜덤 (외부 의존성)// [1] Readable 구현체class RandomStream extends stream.Readable { constructor(options) { super(options); } // Readable에서 구현해야 하는 함수는 _read 하나임 // _read(size) { const chunk = chance.string(); //[1] 랜덤값 생성 console.log(`Pushing chunk of size: ${chunk.length}`); this.push(chunk, 'utf8'); //[2] Encoding을 설정하면 String으로 읽음 if(chance.bool({likelihood: 5})) { //[3] null을 보내면 종료하기로 약속함 this.push(null); } }}// [2] 스트림 사용const randomStream = new RandomStream();randomStream.on('readable', () =&gt; { let chunk; while((chunk = randomStream.read()) !== null) { // [1] 약속한대로 null 이면 읽기 종료 console.log(`Chunk received: ${chunk.toString()}`); }}); 2. Writable 스트림Writable 스트림은 데이터를 생성하는 게 목적이다. (ex) HTTP response 생성 2-1. 사용 예시stream.write 함수를 사용하면 스트림에 내용을 쓸 수 있다. 1234567891011121314// res가 Writable Stream 이다 :)// &quot;Writable Stream은 데이터의 목적지를 나타낸다&quot;는 뜻은// stream.write(내용) 을 쓰면 해당 stream으로 전달된다는 뜻이다.// 전송하는 입장에선 writable이지만, 받는 입장에선 readable로 취급하면 쓰기, 읽기가 각각 되는 것이다.require(&quot;http&quot;) .createServer((req, res) =&gt; { res.writeHead(200, { &quot;Content-Type&quot;: &quot;text/plain&quot; }); while (chance.bool({ likelihood: 95 })) { // 5% 확률로 루프 빠져나오는 코드. res.write(chance.string() + &quot;\\n&quot;); } res.end(&quot;\\nThe end...\\n&quot;); res.on(&quot;finish&quot;, () =&gt; console.log(&quot;All data was sent&quot;)); // 스트림에 finish 이벤트 리스너 등록 후 종료 }) .listen(8080, () =&gt; console.log(&quot;Listening on http://localhost:8080&quot;)); 2-2. Writable 구현 코드(윗 코드와는 상관 없음.) Writable Stream은 _write 함수를 구현하면 된다. 1234567891011121314class ToFileStream extends stream.Writable { constructor() { super({objectMode: true}); } _write (chunk, encoding, callback) { mkdirp(path.dirname(chunk.path), err =&gt; { if (err) { return callback(err); } fs.writeFile(chunk.path, chunk.content, callback); }); }} 2-3. 백 프래셔 예제백 프래셔란 Read보다 Write가 빠를 때 병목이 생기는 것을 방지하는 메커니즘이다. 12345678910111213141516171819202122require(&quot;http&quot;) .createServer((req, res) =&gt; { res.writeHead(200, { &quot;Content-Type&quot;: &quot;text/plain&quot; }); function generateMore() { while (chance.bool({ likelihood: 95 })) { const shouldContinue = res.write( // res.write가 Falsy를 반환하면 내부 버퍼를 다 사용한 것 // (자세한 내용은 나중에 포스팅할 예정) chance.string({ length: 16 * 1024 - 1 }) ); if (!shouldContinue) { console.log(&quot;Backpressure&quot;); // 백 프래셔를 수행해야 하는 시점 return res.once(&quot;drain&quot;, generateMore); // once로 drain 이벤트 핸들러를 등록해 재시작 대기 } } res.end(&quot;\\nThe end...\\n&quot;, () =&gt; console.log(&quot;All data was sent&quot;)); } generateMore(); }) .listen(8080, () =&gt; console.log(&quot;Listening on http://localhost:8080&quot;)); 3. Duplex StreamDuplex Stream은 Readable + Writable 그 이상 그 이하도 아니며 따라서 설명을 생략한다. 4. Transform StreamTransform 스트림은 읽어들인 데이터를 변조해 내보내는 스트림이다. 스트림이니만큼 chunk 단위로 데이터가 오므로 변환에 유의해야 한다. 4-1. 사용 예시123456789const ReplaceStream = require(&quot;./replaceStream&quot;);const rs = new ReplaceStream(&quot;World&quot;, &quot;Node.js&quot;);rs.on(&quot;data&quot;, (chunk) =&gt; console.log(chunk.toString()));rs.write(&quot;Hello W&quot;);rs.write(&quot;orld!&quot;);rs.end();// Hello Node.js 4-2. Transform 구현 코드스트림 상에서 문자열 일부를 치환하는 코드이다. (어렵다.) 123456789101112131415161718192021222324252627282930class ReplaceStream extends stream.Transform { constructor(searchString, replaceString) { super(); this.searchString = searchString; this.replaceString = replaceString; this.tailPiece = ''; } _transform(chunk, encoding, callback) { const pieces = (this.tailPiece + chunk) .split(this.searchString); const lastPiece = pieces[pieces.length - 1]; const tailPieceLen = this.searchString.length - 1; this.tailPiece = lastPiece.slice(-tailPieceLen); pieces[pieces.length - 1] = lastPiece.slice(0,-tailPieceLen); this.push(pieces.join(this.replaceString)); // 여기서의 callback은 각 chunk의 처리가 완료됐음을 알리는 함수이다. callback(); } // 여기서의 callback은 스트림을 종료시키는 함수이다. _flush(callback) { // flush라는 이름 답게 출력되지 않은 데이터의 출력을 수행한다. this.push(this.tailPiece); // 스트림을 종료한다. callback(); }} TODO: (전부 다 책에서 나온 내용) 스트림 간의 Pipelining(조합) 소개 스트림 기반 비동기 제어 소개 (순차/비순차/제한된 비순차) 스트림 fork, merge 스트림 멀티플렉싱, 디멀티플렉싱 소스 코드 출처: Node.js 디자인 패턴 스트림 파트는 내가 스트림에 대한 경험도 거의 없고 책에서 설명하는 내용이 어려워서 내 생각을 넣어 포스팅하기가 매우 어려웠다. 내용을 간략히 정리하는 선에서 마쳐야 할 것 같아 아쉽다.","link":"/ndp-5-stream-2/"},{"title":"5장 Stream API (3&#x2F;3) - Stream을 사용할 때에 순차 실행, 병렬 실행, 제한된 병렬 실행 구현하기","text":"이 글은 Stream을 사용할 때에 순차 실행, 병렬 실행, 제한된 병렬 실행에 대해 다룬다. 또한 독자가 Node.js Stream에 대한 기초 지식이 있음을 전제로 작성되었음을 밝힌다. 참고 자료: Node.js에서의 스트림 자체에 대해서는 5장 Stream API (1/3) - 스트림 개요 및 Readable Stream Stream의 종류 4가지에 대해서는 5장 Stream API (2/3) - Node.js의 4가지 스트림 소개와 사용법 1. 여러 파일을 하나의 파일로 순차적으로 병합하는 방법스트림은 당연하게도 비동기로 작동한다. 여러 개의 Redable Stream이 있고 하나의 Writable Stream이 있을 때, 각 작업들을 순차적으로 수행하는 방법이 있을까? 가능하다. 여러 개의 Readable 을 각각 Writable로 연결하고, Redable에 순서를 지정하면 된다. 아래 코드에 대한 설명은 주석으로 나타나 있으니 주석을 따라가기 바란다. 1234567891011121314151617181920212223242526272829303132333435363738const fromArray = require('from2-array');const through = require('through2');const fs = require('fs');function concatFiles(destination, files, callback) { const destStream = fs.createWriteStream(destination); // fromArray.obj: readableStream of param array. fromArray.obj(files) // readable 끼리 pipe 수행 // through.obj(fn) == through({ objectMode: true }, fn) =&gt; Transform 스트림 반환 // 현재는 through를 많이 사용하지 않아도 괜찮음. .pipe(through.obj((file, enc, done) =&gt; { const src = fs.createReadStream(file); // 파일명을 file로 입력 받음 // src1 =&gt; dest 로 pipe 연결 (pipe 사용 시 자동으로 백 프래셔 수행. src에서 데이터 생산만 하면 됨.) // src1.end // src2 =&gt; dest 로 pipe 연결 // src2.end // ... // dest.end // [끝] // --- // 연결을 요청함. 이벤트 핸들러 등록과 같은 느낌. 실제 스트림 간의 통신은 비동기로 수행됨. src.pipe(destStream, {end: false}); // 이후 src에서 dest로 연결하려면, dest는 종료되지 않아야 함 // --- // 이 파일에 대한 Read Stream이 끝나면, through.obj로 생성하는 Trasnform 스트림의 // callback인 'done' 함수를 호출하게 함. (단순히 params 이름만 바꾼 것임.) src.on('end', done); })) .on('finish', () =&gt; { // WritableStream을 종료함. destStream.end(); // concatFiles 호출자에게 종료를 알림. callback(); });} 2. 순서에 상관 없이 결과를 비동기로, 병렬적으로 한 파일에 출력하는 방법 http://thiswillbedownforsure.com is downhttps://www.naver.com is uphttps://www.google.com is up 위와 같이 특정 사이트 목록들에 대해 health check를 하고 그 결과를 파일로 출력하는 프로그램을 만든다고 하자. 굳이 Stream으로 만들 필요는 없겠지만 그렇게 해본다면 다음과 같은 코드를 생각해볼 수 있다. 일단 Transform 기반의 스트림을 하나 정의한다. 이 스트림은 request의 콜백으로 스트림의 기능을 빌려주는 형태로 작동한다. 123456789101112131415161718192021222324252627282930313233343536373839// Transform 스트림을 하나 정의한다.class ParallelStream extends stream.Transform { constructor(userTransform) { super({objectMode: true}); this.userTransform = userTransform; // const userTransform = (chunk, enc, done, pushFn) =&gt; { ... } this.running = 0; this.terminateCallback = null; } _transform(chunk, enc, done) { this.running++; this.userTransform(chunk, enc, this._onComplete.bind(this), this.push.bind(this)); done(); } // flush는 스트림 종료 직전에 호출되며 즉 done() 의 호출 여부를 결정할 수 있다. _flush(done) { // 작업이 모두 종료되기 전에 스트림이 종료되려고 하는 경우 done()을 호출하지 않는다. // 그 대신 onComplete에서 곧바로 종료할 수 있도록 done 함수를 if(this.running &gt; 0) { this.terminateCallback = done; } else { done(); } } // userTransform에서 done이라는 이름으로 호출되는 함수. 이 때의 done은 각 단위 작업의 완료를 의미한다. _onComplete(err) { this.running--; if(err) { return this.emit('error', err); } // 실행 중인 작업이 모두 종료되었고 스트림 종류가 한 번 이상 보류된 경우 직접 스트림을 종료한다. if(this.running === 0) { this.terminateCallback &amp;&amp; this.terminateCallback(); } }}; 위에서 정의한 스트림을 사용해 구현한다. 123456789101112131415161718192021222324252627282930/*(ex)1. process.argv[2]: urls.txt2. urls.txt:http://thiswillbedownforsure.comhttps://www.naver.comhttps://www.google.com*/fs.createReadStream(process.argv[2]) //[1] 파일로 readable 스트림 생성 .pipe(split()) //[2] 파일의 라인 단위로 chunk를 잡아 출력하는 Transform 스트림 생성 (파일 내용은 url 단위로 줄바꿈 돼있음) .pipe( //[3] pipe로 전달되는 데이터(각 URL) 마다 Transform Stream의 _transform 함수에서 아래의 콜백 함수가 호출된다. // 생성자로 이 콜백(userTransform이라고 불리는)을 등록한다. new ParallelStream((url, enc, done, push) =&gt; { if (!url) return done(); // 더 이상 데이터가 없는 경우 (null인 경우) 스트림 종료하도록 (this.running == 0) request.head(url, (err, response) =&gt; { push(url + &quot; is &quot; + (err ? &quot;down&quot; : &quot;up&quot;) + &quot;\\n&quot;); done(); }); }) ) .pipe(fs.createWriteStream(&quot;results.txt&quot;)) .on(&quot;finish&quot;, () =&gt; console.log(&quot;All urls were checked&quot;)); /* result: http://thiswillbedownforsure.com is down https://www.naver.com is up https://www.google.com is up */ 3. (2)의 동시 실행 수를 제한하는 방법비동기 요청 여러 개를 처리하는 일은 Node.js에선 매우 간단하다. Run to Completion이기 때문에 변수 하나로 비동기 작업의 개수를 정확히 세고 이 값에 기반해 의사 결정을 할 수 있다. 따라서 this.running의 개수가 동시 실행 제한 개수에 도달한 경우 처리하지 않으면 된다. 좀 더 정확하게는, _transform 함수에서 해당 chunk의 처리가 완료됐음을 알리는 콜백을 호출하지 않고 보류하면 된다. 이 경우 해당 chunk를 처리한 결과는 다음 스트림으로 넘어가지 않으며 현재 chunk가 처리되지 않았기 때문에 추가적인 chunk가 스트림으로 전달되지도 않는다(스트림 내부 버퍼에 쌓인다). 만약 ReadableStream이 chunk를 생성하고 내보내는 속도가 우리의 스트림의 처리 속도보다 빠르다면 처리되지 않는 chunk는 Transform의 버퍼에 쌓이며 이내 백 프레셔가 발동되고 알아서 처리될 것이다. - pipe로 연결하면 Node.js에서 자동으로 처리한다. 백 프래셔에 대해선 5장 Stream API (2/3) - Node.js의 4가지 스트림 소개와 사용법을 참고하라. 따라서 추가적으로 신경써야 하는 부분은 출력을 할 지 여부를 결정하는 것이다. 12345678910111213141516171819202122232425262728293031323334353637383940constructor(concurrency, userTransform) { // ... 아래 두 멤버 필드만 추가된다. this.concurrency = concurrency; this.continueCallback = null;}// 입력은 직접 제한하지 않고 계속 받는다.// 완료를 의미하는 콜백을 호출해서 다음 chunk를 처리하지 않으면, 스트림 내부의 버퍼에 쌓이게 된다.// 그러면 Node.js 런타임이 자동으로 백 프레셔를 수행한다._transform(chunk, enc, done) { this.running++; this.userTransform(chunk, enc, this.push.bind(this), this._onComplete.bind(this)); if (this.running &lt; this.concurrency) { done(); } else { // 만약 현재 running의 최대치에 도달한 경우 완료 콜백을 수행하지 않는다. 이는 자연스럽게 백 프래셔 발동으로 이어진다. this.continueCallback = done; }}_onComplete(err) { this.running--; if (err) { return this.emit(&quot;error&quot;, err); } // continueCallback이 할당되어 있으면 호출한다. // 이 시점에서 앞 chunk들은 모두 처리됐음이 보장된다. // 왜냐하면 입력이 출력보다 충분히 빨라 버퍼링이 되는 시점에서는 항상 continueCallback으로 done()이 호출되게 된다. // 항상 this.running == this.concurrency여서 꽉 차 있는 상태이기 때문이다. // (설명이 부드럽지 못한데 실행 흐름을 보고 설명을 다시 읽어보면 이해가 될 것이다.) const tmpCallback = this.continueCallback; this.continueCallback = null; tmpCallback &amp;&amp; tmpCallback(); if (this.running === 0) { this.terminateCallback &amp;&amp; this.terminateCallback(); }} 참고 자료 (이번 글만 특별히 도움이 됐는지와는 별개로 읽은 몇 개의 글을 링크한다.): What’s the proper way to handle back-pressure in a node.js Transform stream? Awesome Nodejs#Streams (Github Repo) TODO: Stream 관련해서 자세한 자료보단 내부 구조를 코드 수준에서 확인하는 게 가장 좋을 것 같다. Back Pressure의 효과를 제대로 확인하기 위해선 디버거를 키고 스트림 객체를 살펴봐야 할 것 같다. Stream의 추상하된 구현체들을 가져다 쓸 수록 더욱 더 이해하기 어려워지는 것 같다. Stream을 3부작으로 나누어 작성하려고 했는데 한 10부작 까지는 나올 수도 있을 것 같다. 그만큼 부족하고, 글 쓰는 데도 매우 오래 걸린다.","link":"/ndp-5-stream-3/"},{"title":"5장 Stream API 디자인 패턴 - Pipe, Fork, Merge, Mux&#x2F;Demux","text":"이 글은 Stream에서의 Pipe, Fork, Merge, Mux/Demux 패턴에 대해 소개하고 Mux/Demux는 예를 제공한다. 참고 자료: Node.js의 Buffer API에 대해서는 Node.js의 Buffer를 제대로 이해해보자 Node.js에서의 스트림 자체에 대해서는 5장 Stream API (1/3) - 스트림 개요 및 Readable Stream Stream의 종류 4가지에 대해서는 5장 Stream API (2/3) - Node.js의 4가지 스트림 소개와 사용법 Stream 기반의 순차 실행, 병렬 실행 구현에 대해서는 5장 Stream API (3/3) - Stream을 사용할 때에 순차 실행, 병렬 실행, 제한된 병렬 실행 구현하기 1. Pipe 패턴여기서 말하는 Pipe 패턴이란 스트림의 조합으로 이루어진 하나의 파이프라인을 모듈화하고 재사용하는 방법을 말한다. Pipe 패턴 구현 시 주의할 점 첫 Stream에 Write하고, 마지막 Stream에서 Read해야 한다. 내부의 모든 Stream에서 발생하는 오류를 포착할 수 있어야 한다. Error Listener 하나로 Pipeline에서 발생하는 모든 오류를 구독할 수 있도록 한다. Combined-Stream 패키지를 이용한다. (사용량은 압도적이나 Stream v1 - Flowing 모드만 지원한다.) (Pumpify가 더 좋은 것 같은데 사용법을 잘 모르겠다.) 2. Fork 패턴서로 다른 대상에 동일한 데이터를 보내는 경우, 즉 하나의 Readable에 2개 이상의 스트림을 연결하는 패턴이다. Fork 패턴 구현 시 주의할 점 .pipe 사용 시 {end: false} 옵션이 필수가 된다. 한 쪽의 작업이 끝나는 경우 다른 쪽도 닫히기 때문 백 프레셔 때문에 제일 느린 스트림에 속도가 맞춰지게 된다. 같은 프로세스 내에 두 스트림이 있는 경우 chunk가 공유되므로 한 쪽의 스트림에서 해당 chunk의 내용을 직접 수정하게 되면 다른 스트림도 그 영향을 받게 된다. 3. Merge 패턴일련의 Readable을 하나의 스트림으로 연결하는 패턴이다. .pipe({end: false})로 연결해야 한다. Auto End 옵션은 하나의 Redable만 종료되더라도 연결된 스트림까지 종료시키기 때문이다. Merge-Stream 패키지를 사용한다. multistream 패키지보다 훨씬 사용량이 많다. 4. Mux/Demux 패턴(직접 구현한다.) 여러 스트림에서 들어오는 데이터를 한 스트림(이 예에서는 net 패키지의 도움을 받아 TCP Socket을 사용한다.)으로 내보내고, 같은 방식으로 데이터를 받아들인 후 여러 스트림으로 다시 분류하는 멀티플렉싱/디멀티플렉싱을 스트림 수준에서 구현한다. 긴 설명은 하지 않고, 코드에 주석을 달아 놓았으니 흐름을 따라가면 쉽게 이해할 수 있을 것이다. generateData.js 표준 출력, 오류 스트림에서 데이터를 생성하기 위한 코드이다. Client에서 실행하게 된다. 12345console.log(&quot;out1&quot;);console.log(&quot;out2&quot;);console.error(&quot;err1&quot;);console.log(&quot;out3&quot;);console.error(&quot;err2&quot;); Client.js generateData로 생성된 데이터가 표준 출력, 오류 스트림으로 들어오게 되고, 아래 코드에서 헤더로 포장한 후 Socket으로 Server에 전송한다. (참고로 Client 코드가 이 case에서 가장 어렵다. 이 코드만 이해하면 다 했다고 볼 수 있다.) 123456789101112131415161718192021222324252627282930313233343536373839404142const child_process = require(&quot;child_process&quot;);const net = require(&quot;net&quot;);function multiplexChannels(sources, destination) { let totalChannels = sources.length; for (let i = 0; i &lt; sources.length; i++) { sources[i] .on(&quot;readable&quot;, function () { let chunk; while ((chunk = this.read()) !== null) { // 5+chunk byte (Node.js는 바이트 스트림(Octet Stream)만 지원하는건지, Buffer도 최소 단위가 bit이 아니라 byte이다.) const outBuff = Buffer.alloc(1 + 4 + chunk.length); // Buffer.alloc(size); === new Buffer(size); outBuff.writeUInt8(i, 0); // write(data, idx) - 이 경우에는 idx=0 outBuff.writeUInt32BE(chunk.length, 1); // write (data, idx) 이 경우에는 idx=1 (앞 데이터는 8bit 이므로, 한 칸만 사용) chunk.copy(outBuff, 5); // 앞에서 40bit를 사용해서 다음 데이터의 offset=5 // chunk가 무슨 타입인지 모르겠지만 Readable이 제공하는 chunk는 copy 메소드가 있는 듯. console.log(&quot;Sending packet to channel: &quot; + i); destination.write(outBuff); // 대상 스트림으로 쓰기 } }) .on(&quot;end&quot;, () =&gt; { // 모든 Readable이 닫힌 후 대상 스트림 종료 if (--totalChannels === 0) { destination.end(); } }); }}// net.connect: (port, host?, callback)const socket = net.connect(3000, () =&gt; { //현재 프로세스는 소켓을 열고 끝. net.connect는 Non-blocking call 이다. const child = child_process.fork( // child_process.fork로 새 프로세스에서 JS 파일을 실행한다. (이 경우 generateData.js) process.argv[2], process.argv.slice(3), // fork로 실행할 JS파일 { silent: true } // silent 옵션: Child 프로세스가 독립적인 표준 스트림을 갖도록 (상속받지 않도록) ); multiplexChannels([child.stdout, child.stderr], socket); // 대상 스트림으로 Socket 생성해 전달}); Server.js 클라이언트로부터 데이터를 파싱한 후 각 스트림에 대응되는 파일에 내용을 쓴다. 헤더 격인 앞 1바이트를 읽어 채널을 구분한다. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758const net = require(&quot;net&quot;);const fs = require(&quot;fs&quot;);// 소켓마다 한 번 호출됨 (상태 관리 필요)// 디멀티플렉싱 수행function demultiplexChannel(source, destinations) { // 상태 관리 필드 let currentChannel = null; let currentLength = null; source .on(&quot;readable&quot;, () =&gt; { // 1. 표준 입력/오류 구분 let chunk; if (currentChannel === null) { chunk = source.read(1); currentChannel = chunk &amp;&amp; chunk.readUInt8(0); } // 2. 데이터 길이 파싱 if (currentLength === null) { chunk = source.read(4); currentLength = chunk &amp;&amp; chunk.readUInt32BE(0); if (currentLength === null) { return; } } // 3. 데이터 길이만큼 읽기 chunk = source.read(currentLength); if (chunk === null) { return; } // 4. 읽은 데이터(chunk)를 대상 스트림에 작성 console.log(&quot;Received packet from: &quot; + currentChannel); destinations[currentChannel].write(chunk); // chunk 순서대로 호출되므로 여기서 다시 null을 할당하면 됨 :) currentChannel = null; currentLength = null; }) .on(&quot;end&quot;, () =&gt; { // 소켓에서 받은 데이터가 끝난 경우 대상 스트림 모두 종료 destinations.forEach((destination) =&gt; destination.end()); console.log(&quot;Source channel closed&quot;); });}// 소켓 3000번으로 서버 열기net .createServer((socket) =&gt; { // 연결 수립 시 수행할 Callback const stdoutStream = fs.createWriteStream(&quot;stdout.log&quot;); const stderrStream = fs.createWriteStream(&quot;stderr.log&quot;); // Source: 소켓을 통해 전달된 octet-stream, 대상 스트림 2개: 표준 출력, 표준 오류 demultiplexChannel(socket, [stdoutStream, stderrStream]); }) .listen(3000, () =&gt; console.log(&quot;Server started&quot;)); TODO: 스트림을 제대로 써봐야 제대로 이해할 수 있을 것 같다. 스트림 생태계가 좀 엉망인데 직접 사용해보고 정리하는 기회가 필요할 것 같다. 이 글도 예제를 제대로 추가해 영양가 있는 글로 만들어야 한다.","link":"/ndp-5-stream-4/"},{"title":"7장 의존성 주입 (1&#x2F;2) - Node.js&#x2F;Javascript 환경에서의 한 패키지 내의 의존성 관리","text":"이 글은 Node.js/Javascript 환경에서의 한 패키지(App) 내의 모듈 간의 의존성을 관리하는 방법에 대해 다룬다. 명시하지 않은 경우 Javascript 환경임을 미리 밝힌다. Typescript는 지금까지 많이 활용돼왔고 생태계가 성숙한 상태이므로, OOP 방식으로 문제 해결을 하려는 경우 Typescript가 적정 기술이라고 생각한다. 1. 유독 언급이 적은 Node.js에서의 의존성 관리, 왜?백엔드와 같이 쉽고 빠르게 규모가 커지고 기능 변경이 잦은 코드 베이스인 경우 설계가 중요한 경우가 많을 것이다. 설계는 의존성 관리가 기본이며 Node.js 백엔드 또한 그 예외는 아닐 것인데 말이다. Q. 왜 Node.js에서는 하드 코딩된 의존 관계를 구축하는 코드를 찾기가 매우 쉬울까? A. 가설: 인터페이스와 상관 없이 임의의 객체를 집어 넣어 테스트를 할 수 있기 때문에 굳이 Interface가 필요하지 않다. 동적 타입 언어이니까. 2. “동적 타입 언어”라는 특징장점만약 Java 였다면 Interface를 아예 사용하지 않는 것은 설계에 큰 문제가 있음을 시사하는 것이겠지만 Javascript는 동적 타입 언어이다. 기능을 실행하는 객체의 타입이 중요하지 않은 언어이다. 인터페이스가 없는 만큼 규칙도 없지만 그만큼 유연해진 셈이다. 단점다만 동적 타입을 활용해 테스트가 가능하다고 해도 자연스럽게 생기는 강한 결합이 사라지는 것은 아니다. 구현체에 직접 의존하면 강한 결합이 발생한다. 의존하는 객체의 구현 상세에 대한 아무런 격리 장치가 없으며 구현체에서 변경이 생겼을 때 해당 의존성을 사용하는 모든 객체에 그 여파가 전달되므로 다시 검증(테스트), 빌드해야만 한다. 3. OOP의 문제 해결 방식OOP 에서는 인터페이스를 미리 정의하고 해당 인터페이스를 최대한 변경하지 않음(Open Close Principle)을 통해 문제를 해결한다. 인터페이스에 의존함을 통해 구현 상세와 사용 객체를 진정으로 격리시킬 수 있으며 이는 의존성 관리에 매우 큰 역할을 한다. 의존성에 의한 강한 결합을 막는 수단은 현재로썬 서비스 로케이터 패턴과 의존성 주입이 있다. 이제부터 이 글은 Javascript/Node.js 에서의 의존성 주입에 대해 다룬다. 4. 서비스 로케이터 패턴서비스 로케이터 패턴이란 “의존성이 있는 각 객체가 서비스 로케이터 객체만을 직접 의존하고, 각 객체는 서비스 로케이터에 의존성을 명시해 구현체를 받아오는 것“을 말한다. (서비스 로케이터 패턴에 대해 더 자세히 알고 싶다면 이 글을 참고하라.) 아래 예는 AuthController가 AuthService에 의존하는 코드이다. 123456789// AuthController.js - AuthService에 의존한다.// AuthController는 ServiceLocator에만 '직접' 의존한다.module.exports = (serviceLocator) =&gt; { const authService = serviceLocator.get('authService'); // TS 등 정적 타입 언어에서는 타입으로 받아온다. // Javascript는 딱히 타입이 없으므로 String으로 의존성(객체)을 식별한다. // require()와 사용 방식이 매우 닮아있다. 차이가 있다면, require는 전체 경로를 명시한다는 점이다. const authController = {}; //...} 서비스 로케이터 패턴의 장점의존성의 구현체에 의존하지 않게 해준다. 이는 의존성 주입과 동일한 장점이며 아주 좋은 장점이다. 서비스 로케이터 패턴의 단점객체의 구현 코드를 보지 않으면 곧바로 의존 관게를 파악할 수 없다. 생성자 등으로 명시하지 않기 때문에 - 생성자의 파라미터로 명시한다면 필수값이라는 문서화의 역할을 수행하게 되는데 비해 - 모든 객체에 대해 문서화가 필요하다. 5. 의존성 주입의존 관계를 가장 잘 다루는 방법은 아마도 DI일 것이다. Javascript 진영에선 Angular가 최초로 의존성 주입을 도입한 것으로 안다(Typescript도 없던 시절이었는데!). 의존성 주입이란 “모듈의 의존성을 외부 개체에 의해 입력으로 전달 받는 것“을 말한다. 의존성 주입의 개념 자체는 매우 간단하다. DI를 지원하기 위한 컨테이너와 지원 방식을 구현하는 게 어려울 뿐이다. (ex) AuthController가 AuthService에 의존하는 경우의 예시를 확인하자. Before DI: 구현체를 직접 가져오는 모듈 1234567// 직접 가져온다.const authService = require('./authService');exports.login = (req, res, next) =&gt; { authService.login(...); //...}; After DI: 의존성을 받아오는 모듈 12345678910// authService를 전달 받아서 사용한다. authService의 출처와 구현체에 대해 아는 것은 더 이상 이 객체의 책임이 아니다. 그냥 사용만 하면 된다.module.exports = (authService) =&gt; { const authController = {}; authController.login = (req, res, next) =&gt; { authService.login(req.body.username, req.body.password, ...); //... } return authController}; Service Locator / DI Container의 간략한 구현도 포함하려고 했으나 2편에서 다루도록 하겠다. 6. Node.js의 DI 컨테이너 생태계약간의 짬을 내어 찾아보니 크게 4개의 오픈소스 컨테이너들이 있었다: InversifyJs, tsyringe, typedi, awilix (점유율 순). tsyringe는 Microsoft에서 만들었다. 재밌는 점은 MS에서 inversifyjs를 사용한다고 나와있는 것이다. NestJs는 DI를 Core에 내장하여 차트에 포함시켰다. 각 라이브러리의 자세한 비교는 기회가 된다면 추후 진행하려 한다. TODO: Clean Architecture를 다시 읽는다. SOLID 원칙 조차 희미해진 듯하다. DI와 DIP의 관계에 대해 다시 공부해야겠다. 양파 껍질 Architecture에 대해 제대로 이해해야겠다. require과 서비스 로케이터 패턴의 관계에 대해 이해해야겠다. CS에서 가장 자신있던 객체지향을 이렇게 모르게 됐다는 게 새삼 충격적이다 :(","link":"/ndp-7-dependency-injection-1/"},{"title":"7장 의존성 주입 (2&#x2F;2) - 간단한 Javascript DI 컨테이너 구현체","text":"이 글은 7장 의존성 주입 (1/2)에서 설명한 DI 컨테이너의 간단한 구현체를 제시한다. Javascript이기 때문에 타입 정보를 얻을 수 없어 String으로 의존성을 판단하는 부분을 참고하기 바란다. 이 글의 코드는 출처에서 배포된 코드를 가져왔음을 밝힌다. 1. DI 컨테이너 구현diContainer.js아쉽게도 패키지 전체를 미리 스캔하여 자동으로 의존 관계를 파악하고 의존성 주입을 수행하지는 않는다. 기능은 크게 get, factory, register가 있다. 자세한 설명은 주석을 참고하라. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354&quot;use strict&quot;;// fnArgs는 함수의 인자 목록을 String 배열로 반환한다.const fnArgs = require(&quot;parse-fn-args&quot;);module.exports = () =&gt; { const dependencies = {}; const factories = {}; const diContainer = {}; // factory, register 둘 다 단순 등록 기능이다. // factory 메소드의 경우 의존성 주입이 필요한 객체인 경우 사용한다. diContainer.factory = (name, factory) =&gt; { factories[name] = factory; }; // register 메소드의 경우 의존성 주입이 필요 없는 객체(상수 등)를 등록할 때 사용한다. diContainer.register = (name, dep) =&gt; { dependencies[name] = dep; }; /* 1. get은 dependencies에 없는 경우 factory로 간주하고 가져옴 2. 만약 가져오려 했던 객체가 존재하면 해당 객체로 inject를 호출함 (inject를 통해 재귀적으로 의존성을 resolve.) 3. (2)의 결과를 dependencies에 저장 4. 만약 그래도 dependencies에 없는 경우 모듈을 찾을 수 없는 것. */ diContainer.get = (name) =&gt; { if (!dependencies[name]) { const factory = factories[name]; dependencies[name] = factory &amp;&amp; diContainer.inject(factory); if (!dependencies[name]) { throw new Error(&quot;Cannot find module: &quot; + name); } } return dependencies[name]; }; /* 1. factory로 등록된 객체를 전달받음 2. fnArgs는 함수(factory의 경우, 의존성을 명시한 함수를 export 함.)의 인자를 가져옴 3. 인자에 대해 map으로 get을 수행한 배열을 args 변수에 저장함 4. factory(생성자)를 resolved 된 dependencies로 호출함 */ diContainer.inject = (factory) =&gt; { const args = fnArgs(factory).map(function (dependency) { return diContainer.get(dependency); }); return factory.apply(null, args); }; return diContainer;}; 2. 컨테이너 사용1. app.jsDI 컨테이너에 각 객체를 등록하는 과정을 이 파일을 진입점 삼아 수행하였다. 좀 더 좋은 DI 컨테이너라면 Reflection 등을 이용해 자동으로 mark된 객체를 등록하고 의존성 주입을 진행할 것이다. 12345678910111213141516171819202122232425&quot;use strict&quot;;//...const diContainer = require(&quot;./lib/diContainer&quot;)();// register는 추가적으로 의존성 주입이 필요 없는 객체를 등록한다. (상수 등)diContainer.register(&quot;dbName&quot;, &quot;example-db&quot;);diContainer.register(&quot;tokenSecret&quot;, &quot;SHHH!&quot;);// factory는 의존성 주입이 필요한 객체를 등록한다.diContainer.factory(&quot;db&quot;, require(&quot;./lib/db&quot;));// Service 객체 등록 (의존성 주입 필요한 상태)diContainer.factory(&quot;authService&quot;, require(&quot;./lib/authService&quot;));// Controller 객체 등록 (의존성 주입 필요한 상태)diContainer.factory(&quot;authController&quot;, require(&quot;./lib/authController&quot;));// get은 의존성을 반환한다. (재귀적으로 의존성 주입이 된 채로 반환된다.)const authController = diContainer.get(&quot;authController&quot;);// Express에 Controller 등록app.post(&quot;/login&quot;, authController.login);app.get(&quot;/checkToken&quot;, authController.checkToken);//... 2. authController.js의존성 주입이 적용되는 객체 1이다. 주석 참고. 12345678910111213141516171819202122232425262728293031323334353637&quot;use strict&quot;;// Express에서 일반적으로 사용되는 Controller 예제이다.// 모듈 차원에서 함수로 내보내며(DI 컨테이너 작동 방식에 맞춤), 인자에 이름으로 의존성을 명시한다.module.exports = (authService) =&gt; { // DI 컨테이너에 의해 authService 의존성을 주입 받게 된다. const authController = {}; authController.login = (req, res, next) =&gt; { authService.login(req.body.username, req.body.password, (err, result) =&gt; { if (err) { return res.status(401).send({ ok: false, error: 'Invalid username/password' }); } res.status(200).send({ok: true, token: result}); } ); }; authController.checkToken = (req, res, next) =&gt; { authService.checkToken(req.query.token, (err, result) =&gt; { if (err) { return res.status(401).send({ ok: false, error: 'Token is invalid or expired' }); } res.status(200).send({ok: 'true', user: result}); } ); }; return authController;}; 3. appService.js의존성 주입이 적용되는 객체 2이다. 주석 참고. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748&quot;use strict&quot;;const jwt = require('jwt-simple');const bcrypt = require('bcrypt');// 역시 모듈을 함수로 내보내며 의존성을 명시했다.// 이 예제에서는 db 객체에 대해선 생략하였다. (DI 설명에서 의미 없는 구성 요소)module.exports = (db, tokenSecret) =&gt; { const users = db.(...); const authService = {}; authService.login = (username, password, callback) =&gt; { users.get(username, (err, user) =&gt; { if (err) return callback(err); bcrypt.compare(password, user.hash, (err, res) =&gt; { if (err) return callback(err); if (!res) return callback(new Error('Invalid password')); const token = jwt.encode({ username: username, expire: Date.now() + (1000 * 60 * 60) //1 hour }, tokenSecret); callback(null, token); }); }); }; authService.checkToken = (token, callback) =&gt; { let userData; try { userData = jwt.decode(token, tokenSecret); if (userData.expire &lt;= Date.now()) { throw new Error('Token expired'); } } catch(err) { return process.nextTick(callback.bind(null, err)); } users.get(userData.username, (err, user) =&gt; { if (err) return callback(err); callback(null, {username: userData.username}); }); }; return authService;}; TODO: Node.js 스트림 이어서 포스팅하기","link":"/ndp-7-dependency-injection-2/"}],"tags":[{"name":"Frontend","slug":"Frontend","link":"/tags/Frontend/"},{"name":"React.js","slug":"React-js","link":"/tags/React-js/"},{"name":"CI","slug":"CI","link":"/tags/CI/"},{"name":"CI&#x2F;CD","slug":"CI-CD","link":"/tags/CI-CD/"},{"name":"GitHub Actions","slug":"GitHub-Actions","link":"/tags/GitHub-Actions/"},{"name":"Docker","slug":"Docker","link":"/tags/Docker/"},{"name":"1 Month Docker","slug":"1-Month-Docker","link":"/tags/1-Month-Docker/"},{"name":"ExpressJs","slug":"ExpressJs","link":"/tags/ExpressJs/"},{"name":"함수형 자바스크립트","slug":"함수형-자바스크립트","link":"/tags/%ED%95%A8%EC%88%98%ED%98%95-%EC%9E%90%EB%B0%94%EC%8A%A4%ED%81%AC%EB%A6%BD%ED%8A%B8/"},{"name":"Javascript","slug":"Javascript","link":"/tags/Javascript/"},{"name":"NestJs","slug":"NestJs","link":"/tags/NestJs/"},{"name":"Linux","slug":"Linux","link":"/tags/Linux/"},{"name":"Node.js 디자인 패턴","slug":"Node-js-디자인-패턴","link":"/tags/Node-js-%EB%94%94%EC%9E%90%EC%9D%B8-%ED%8C%A8%ED%84%B4/"}],"categories":[{"name":"Frontend","slug":"Frontend","link":"/categories/Frontend/"},{"name":"CI&#x2F;CD","slug":"CI-CD","link":"/categories/CI-CD/"},{"name":"Docker","slug":"Docker","link":"/categories/Docker/"},{"name":"React.js","slug":"Frontend/React-js","link":"/categories/Frontend/React-js/"},{"name":"ExpressJs","slug":"ExpressJs","link":"/categories/ExpressJs/"},{"name":"함수형 자바스크립트","slug":"함수형-자바스크립트","link":"/categories/%ED%95%A8%EC%88%98%ED%98%95-%EC%9E%90%EB%B0%94%EC%8A%A4%ED%81%AC%EB%A6%BD%ED%8A%B8/"},{"name":"Javascript","slug":"Javascript","link":"/categories/Javascript/"},{"name":"NestJs","slug":"NestJs","link":"/categories/NestJs/"},{"name":"Linux","slug":"Linux","link":"/categories/Linux/"},{"name":"Node.js 디자인 패턴","slug":"Node-js-디자인-패턴","link":"/categories/Node-js-%EB%94%94%EC%9E%90%EC%9D%B8-%ED%8C%A8%ED%84%B4/"}]}